\section{Other Considered Things}
\subsection{Typing with a products Category (and a bit of polymorphism)}
\label{app:prodcat}
Another way to start would be to consider product categories: one for the main type system and one for the effects.
Let $\mC_{0}$ be a closed cartesian category representing our main type system.
Here we again consider constants and full computations as functions $\bot \to \tau$ or $\tau \in \mathrm{Obj}\left( \mC_{0} \right)$.
Now, to type functions and functors, we need to consider a second category:
We consider $\mC_{1}$ the category representing the free monoid on $\mF\left( \mL \right)$.
Monads and Applicatives will generate relations in that monoid.
To ease notation we will denote \emph{functor types} in $\mC_{1}$ as lists written with head on the left.

Finally, let $\mC = \mC_{0} \times \mC_{1}$ be the product category. This will be our typing category.
This means that the real type of objects will be $\left( \bot \to \tau, [] \right)$, which we will still denote by $\tau$.
We will denote by $F_{n} \cdots F_{0} \tau$ the type of an object, as if it were a composition of functions\footnote{It is!}.

In that paradigm, functors simply append to the head of the \emph{functor type} (with the same possible restrictions as before, though I do not see what they would be needed for) while functions will take a polymorphic form:
$x: L\tau_{1} \mapsto \phi x: L\tau_{2}$ and $\phi$'s type can be written as $\star\tau_{1} \to \star\tau_{2}$.

\subsection{Multiple Ways}
\label{app:arities-and-denots}
In this section we will discuss what can happen when the denotation system
we consider is not actually based on a compositional model, or is independent
of the syntactic model.
In that case, our model of using typing to retrieve parsing trees will need
modifications to be integrated.

In the case where the compositional aspects of the sentence are blurred by a
non-binary arity, whether it's variable (leading to non-binary trees) or simply
non-compositional and makes use of all the words of the sentence (or the ones
prior to the last one read, like in a LLM), we can make use of higher arity
base combinators.
We replace the function application by the combinator adapted to our model,
then a theoretical (or learned) analysis of the concepts that can be modeled
by adding effects can still be done, and our work can be completed in the way
described above.

Of course in the case where no syntactic structure is used for semantics,
using semantics to derive syntax is useless, but the formalism of string
diagrams used to present the actual parsing can still be useful:
it could also be used to model the attention patterns (as side-effects of the
reading of a new token and adding its meaning to the current representation
of the sentence).
This is an insight of how to implement our enhancement onto a non-compositional
system, although that of course removes a bit of the interest of the system, as
typing will not matter in that case.
