\section{Efficient Semantic Parsing}
\label{sec:parsing}
In this section we explain our algorithms and heuristics for efficient semantic
parsing with as little non-determinism as possible, and reducing time complexity
of our parsing strategies.

\subsection{Na√Øve Semantic Parsing on Syntactic Parsing}
In this section we suppose that we have a set of syntax trees (or parsing trees) corresponding to a certain
sentence.
We will now focus on how to derive proofs of typing from a syntax tree.
First, note that \cite{bumfordEffectdrivenInterpretationFunctors2025} provides a way to do so by constructing
semantic trees labelled by sequence of \emph{combinators}.
In our formalism, this amounts to constructing proof trees by mapping combination modes to their equivalent proof
trees, inverting if needed the order of the presuppositions to account for the order of the words.
Computing one tree is easily done in linear time in the number of nodes in the parsing tree (which is linear in the input size,
more on that in the next section), multiplied by a constant whose size depends on the size of the inference
structure.
The main idea is that to each node of the tree, both nodes have a type and there is only a finite set of rules
that can be applied, provided by the following rules, which are a rewriting of Figure \ref{tab:judgements}.
In Figure \ref{tab:proof-trees} we provide \emph{matching-like} rules for different possibilities on the types to
combine and the associated possible proof tree(s).
Note that there is no condition on what the types \emph{look like}, they can be effectful.
If multiple cases are possible, all different possible proof trees should be added to the set of derivations: the
set of proof trees for the union of cases is the union of set of proof trees for each case, naturally:
\begin{equation*}
	PT\left( \cup C \right) = \cup PT(C)
\end{equation*}
Remember again that the order of presuppositions for an inference structure does not matter, so always we have the
other order of arguments available.

\begin{figure*}
	\centering
	\input{aux/figures/match-judgement}
	\caption{List of possible combinations for different presuppositions for inputs, as a definition of a function
		$PT$ from proof trees to proof trees.}
	\label{tab:proof-trees}
\end{figure*}

It is important to note that here the proof trees are done ``in the wrong direction'', as they are written
top-down instead of bottom-up in the sense that we start by the axioms which are the typing judgements of
constants in the lexicon (this could actually be said to be a part of the sentence, but non-determinism in the
meanings is fine as long as the syntactic category is provided in the parsing tree).
This leads to proof trees being a bit weird to the type theorist and a bit weird to the linguist as they are
not written as usual trees.
Moreover, while a proof tree only provides a type, the tree also provides the sequence of function applications
that is needed to compute the actual denotation.

This leads the induced algorithm (for computing the set of denotations) to be exponential in the input in the
worst case, when using the recursive scheme that naturally arises from the definition of $PT$.
Indeed, and while this may seem weird, in the case of a function $\f{F}\ta \to \tb$ where $\f{F}$ is applicative
and the other combinator is of type $\f{F}\ta$, one might apply the function directly or apply it under the $\f{F}$
of the argument, leading to two different proof trees:
\begin{align*}
	\resizebox{\textwidth}{!}{$\prooffrac{\cont \f{F}: \mC \Rightarrow \mC \poulpe \Junit{\f{F}}{l}{\ta}{r}{\tb}}{\cont \f{F}lr: \f{F}\tb}$} \\
	\Japp{l}{\f{F}\ta}{\tb}{r}
\end{align*}
Those two different proof trees have different semantic interpretations that may be useful, as discussed by
\newcite{bumfordEffectdrivenInterpretationFunctors2025}, especially in their analysis of closure properties, which
is modified in the following section.

\subsubsection{Islands}
\newcite{bumfordEffectdrivenInterpretationFunctors2025} provide a formal analysis of islands\footnote{Syntactic
	structures which prevent some notion of moving outside of the island.} based on a islands being a different type
of branching nodes in the syntactic tree of a sentence, which asks to resolve all $\f{C}$ effects\footnote{Those
	represent continuations.} before that node or being resolved at that node.
The main example they propose is the one of existential closure inside conditioning.
To reconcile this inside our type system, we propose the following change to their formalism: once the syntactic
information of an island existing is added to the tree (or at semantic parsing time, this does not change the
time complexity), we \emph{mark} each node inside the island by adding an ``void effect'' to it, in the same
way as we did for our model of plural.
This translates into a functor which just maintains the \emph{island marker} on \texttt{fmap} and add to the
words which create an island node\footnote{Or \emph{post-compose} the proof trees with a rule on handling the
	$\f{C}$ effect.} a function which handles the $\f{C}$ effects, which could be seen as adding a node in the
semantics parse tree from the syntactic parse tree.
A way to do this would be the following: we pass all functors until finding a $\f{C}$, handle it inside the other
functors and keep going, on both sides, where $\mathrm{PT}$ is defined in Figure \ref{tab:proof-trees}:
\begin{equation*}
	\mathrm{PT}\left(
	\suppfrac{\raisebox{-5pt}{$\cont x_{1}: \f{F}\f{C}\f{F'}\tau$}}{\cont x_{1}: \f{F}\f{F'}\tau}\suchthat
	\suppfrac{\raisebox{-5pt}{$\cont x_{2}: \f{F}\f{C}\f{F'}\tau$}}{\cont x_{2}: \f{F}\f{F'}\tau}
	\right)
\end{equation*}
Note that this preserves the linear size of the parse tree in the number of input words, as we at most double the
number of nodes, and note that this would be preserved with additional similar constructs.

\medskip

This idea amounts to seeing islands, whatever their type, as a form of grammatical/syntactic effect, which is
still part of the language but not of the lexicon, a bit like future, modality or plurals, without the semantic
component.
The idea of seeing everything as effects, while semantically void, allows us to translate into our theory of
type-driven semantics outer information from syntax, morphology or phonology that influences the semantics of the
sentence.
Other examples of this can be found in the modelisation of plural (for morphology, see Sections \ref{par:higherorder}
and \ref{subsec:modality}) and the emphasis of the words by the $\f{F}$ effect (for phonology), and show the
empirical well-foundedness of this point of view.
While we do not aim to provide a theory of everything linguistics through categories\footnote{``\emph{A theory is as
		large as its most well understood examples}''.}, the idea of expressing everything in our effect-driven type-driven
theory of semantics allows us to prepare for any theoretical or empirical observation that has an impact on the
semantics of a word/sentence, the allowed combinators and even the addition of rules.

\subsubsection{Rewriting Rules}
\label{subsec:rewrite}
Here we provide a rewriting system on derivation trees that allows us to improve our time complexity.
In the worst case, in big o notation there is no improvement, but there is no loss.
The goal is to reduce branching in the definition of $PT$.

\medskip

First, let's consider the reductions proposed in Section \ref{sec:nondet}.
Those define normal forms under Theorem \ref{thm:confluence} and thus we can use those to reduce the number
of trees.
Indeed, we usually want to reduce the number of effects by first using the rules \emph{inside} the language,
understand, the adjunction co-unit and monadic join.
Thus, when we have a presupposition of the form:
\begin{equation*}
	\suppfrac{\blank}{\cont x: \f{M}\f{M}\tau} \poulpe \text{or} \poulpe \suppfrac{\blank}{\cont x: RL\tau}
\end{equation*}
we always simplify it directly, as not always doing it would propagate multiple trees.
This in turn means we always verify the derivational rules we set up in the previous section for the join
equations of the monad.

\medskip

Secondly, while there is no way to reduce the branching proposed in the previous section since they end in
completely different reductions, there is another case in which two different reductions arise:
\begin{equation*}
	\PT{\blank}{\cont x: \f{F}\tau_{1}} \poulpe \text{and} \poulpe \PT{\blank}{\cont y: \f{G}\tau_{2}}
\end{equation*}
Indeed, in that case we could either get a result with effects $\f{F}\f{G}$ or with effects $\f{G}\f{F}$.
In general those effects are not the same, but if they happen to be, which trivially will be the case when one of
the effects is external, the plural or islands functors for example.
When the two functors commute, the order of application does not matter and thus we choose to get the outer
effect the one of the left side of the combinator.
Note that this is also true when we have an equation that

\medskip

The observant reader might have noticed that in the definition of $PT$, we actually do not cover all the possible
cases, as in particular, we have not included the option of using the unit of an applicative/monad alone, as it
actually would reduce to equivalent cases most of times.
Note moreover that this is simply of scheme to apply typing rules to a syntactic derivation, but that this will not
be enough to actually gain all reductions possible.
This will actually happen once a confluent reduction scheme is provided for the denotations.
When this is done, we can combine the reduction schemes for effects along with the one for denotation and the one
for combinations in one large reduction scheme.
Indeed, trivally, the tensor product of confluent reduction schemes forms a confluent reduction scheme, whose maximal
reduction length is the sum of the maximal reduction lengths\footnote{Actually it is at most that, but that does not
	really matter.}.

\subsubsection{Measuring Improvements}
Since all our improvements cannot remove the inherent exponentiality of the
algorithm behind, let's see if this exponentiality is actually an issue.
Yes, it is!

\subsection{Syntactic-Semantic Parsing}
\label{subsec:ssparsing}
If using a na√Øve strategy on the trees yields an exponential algorithm,
the best way to improve the algorithm, is to directly leave the na√Øve strategy.
To do so, we could simply extend the grammar system used to do the syntactic
part of the parsing.
In this section, we will take the example of a CFG since it suffices to create
our typing combinators,
For example, we can increase the alphabet to be the free monoid on the product
$\Sigma$ of the usual base alphabet and $\bar{\mC}$ our typing category.
Since the usual syntactic structure and our combination modes can both be
expressed by CFGs (or almost, cf Figure \ref{fig:combination-cfg}), our product
system can be expressed by (almost) a CFG.
Here, we change our notations from the proof trees of Figure
\ref{tab:proof-trees} to have a more explicit grammar of combination modes
provided below is a rewriting of the proof trees provided earlier, and
highlights the combination modes in a simpler way, based on
\cite{bumfordEffectdrivenInterpretationFunctors2025} as it simplifies the
rewriting of our typing judgements in a CFG\footnote{That, in a sense, was
	already implicitly provided in Figure \ref{tab:judgements}.}.
The grammars provided  in Figures \ref{fig:english-cfg} and \ref{fig:combination-cfg}
are actually used from right to left, as we actually do combinations over the
types and syntactic categories of the words and try to reduce the sentence, not
derive it from nothing.
Now, all the improvements discussed in Section \ref{subsec:rewrite}
can still be applied here, just a bit differently, as this amounts to reducing
ambiguity in the grammar.

\begin{figure*}
	\centering
	\inputtikz{combination-cfg}
	\caption{Possible Type Combinations in the form of a near CFG}
	\label{fig:combination-cfg}
\end{figure*}

We do not prove here that these typing rules cannot be written in a simpler
syntactic structure\footnote{It is important to note that for a typing system,
	we only have syntactic rules that allow, or not, to combine types.}, but it is
easy to see why a regular expression would not work, and since we need trees,
to express our full system, the best we can do would be to disambiguate the
grammar.
A thing to note is that this grammar is not complete but explains how types can
be combined in a compact form.
For example, the rules of the form $\combML_{\f{F}} M, \tau' \gets M, \tau$ are
rules that provide ways to combine effects from the two inputs in the order we
want to: we can combine $\f{R}\f{S}\e$ and $\f{C}\f{W}(\e \to \t)$ into
$\f{R}\f{C}\f{W}\f{S}\t$ with the mode $\combML\combMR\combMR\combML <$ (see
\cite{bumfordEffectdrivenInterpretationFunctors2025} Example 5.14).

\medskip

Moreover, it is important to note that while we talk about the structure in
Figure \ref{fig:combination-cfg} as a grammar, it is more of a structure that
computes if it is possible or not to combine two types and how.
In particular, our grammar is not finite, since there are infinitely many types
that can be playing the part of $\alpha$ and $\beta$, and infinitely many
functors that can play the roles for $\f{F}$ and so on, but that does not pose
a problem, as recognizing the general form of a type can be done in constant
time for the forms that we want to check, given a proper memory representation.
However, we still need to find a way to render that structure close enough of
a CFG to keep a polynomial computation time, as that will be the basis for
Theorem \ref{thm:ptime-parse}.

\begin{thm}
	\label{thm:ptime-parse}
	Semantic parsing of a sentence is polynomial in the length of the	sentence
	and the size of the type system and syntax system.
\end{thm}
\begin{proof}
	Suppose we are given a syntactic generating structure $G_{s}$ along with our
	type combination grammar $G_{\tau}$.
	The system $G$ that can be constructed from the (tensor) product of $G_{s}$ and
	$G_{\tau}$ has size $\abs{G_{s}}\times \abs{G_{\tau}}$.
	Indeed, we think of its rules as a rule for the syntactic categories and a rule
	for the type combinations. Its terminals and non terminals are also in the
	cartesian products of the adequate sets in the original grammars.
	What this in turn means, is that if we can syntactically parse a sentence in
	polynomial time in the size of the input and in $\abs{G_{s}}$, then we can
	syntactico-semantically parse it in polynomial time in the size of the input,
	$\abs{G_{s}}$ and $\abs{G_{\tau}}$.
\end{proof}

While we have gone with the hypothesis that we have a CFG for our language,
any type of polynomial-time structure could work\footnote{Simply, the algorithm
	will need to be	adapted to at least process the CFG for the typing rules.},
as long as it is more expressive than a CFG.
We will do the following analysis using a CFG since it allows to cover enough of
the language for our example and for simplicity of describing the process of
adding the typing CFG, even though some think that English is not a context
free language \cite{higginbothamEnglishNotContextFree1984}.
Since the CYK algorithm provides us with an algorithm for CFG based parsing
cubic in the input and linear in the grammar size, we end up with an algorithm
for semantically parsing a sentence that is cubic in the length of the sentence
and linear in the size of the grammar modeling the language and the type system.

\begin{thm}
	\label{thm:ptime-denot}
	Retrieving a pure denotation for a sentence can be done in polynomial time in
	the length of the sentence, given a polynomial time syntactic parsing
	algorithm and polynomial time combinators.
\end{thm}
\begin{proof}
	We have proved in Theorem \ref{thm:ptime-parse} that we can retrieve a
	semantic parse tree from a	sentence in polynomial time in the input.
	Since we also have shown that the length of a semantic parse tree is quadratic
	in the length of the sentence it represents, being linear in the length of a
	syntactic parse tree linear in the length of the sentence.
	We have already seen that given a denotation, handling all effects and
	reducing effect handling to normal forms can be done in polynomial time.
	The superposition of these steps yields a polynomial-time algorithm in the
	length of the input sentence.
\end{proof}

The \emph{polynomial time combinators} assumption is not a complex assumption,
this is for example true for our denotations in Section \ref{sec:language},
with function application being linear in the number of uses of variables in
the function term, which in turn is linear in the number of terms used to
construct the function term and thus of words, and \fmap being in constant
time\footnote{Depending on the functor but still bounded by a constant.} for
the same reason.
Of course, function application could be said to be in constant time too, but
we consider here the duration of the $\beta$-reduction:
\begin{equation*}
	\left(\lambda x. M\right)N \xrightarrow{\beta} M\left[x / N\right]
\end{equation*}



