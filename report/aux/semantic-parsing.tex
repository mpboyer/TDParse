\section{Efficient Semantic Parsing}
\label{sec:parsing}
In this section we explain our algorithms and heuristics for efficient semantic
parsing with as little non-determinism as possible, and reducing time complexity
of our parsing strategies.

\subsection{Naïve Semantic Parsing on Syntactic Parsing}
In this section we suppose that we have a set of syntax trees (or parsing trees) corresponding to a certain
sentence.
We will now focus on how to derive proofs of typing from a syntax tree.
First, note that \cite{bumfordEffectdrivenInterpretationFunctors2025} provides a way to do so by constructing
semantic trees labelled by sequence of \emph{combinators}.
In our formalism, this amounts to constructing proof trees by mapping combination modes to their equivalent proof
trees, inverting if needed the order of the presuppositions to account for the order of the words.
Computing one tree is easily done in linear time in the number of nodes in the parsing tree (which is linear in the input size,
more on that in the next section), multiplied by a constant whose size depends on the size of the inference
structure.
The main idea is that to each node of the tree, both nodes have a type and there is only a finite set of rules
that can be applied, provided by the following rules, which are a rewriting of Figure \ref{tab:judgements}.
In Figure \ref{tab:proof-trees} we provide \emph{matching-like} rules for different possibilities on the types to
combine and the associated possible proof tree(s).
Note that there is no condition on what the types \emph{look like}, they can be effectful.
If multiple cases are possible, all different possible proof trees should be added to the set of derivations: the
set of proof trees for the union of cases is the union of set of proof trees for each case, naturally:
\begin{equation*}
	PT\left( \cup C \right) = \cup PT(C)
\end{equation*}
Remember again that the order of presuppositions for an inference structure does not matter, so always we have the
other order of arguments available.

\begin{figure}
	\centering
	\input{aux/figures/match-judgement}
	\caption{List of possible combinations for different presuppositions for inputs, as a definition of a function
		$PT$ from proof trees to proof trees.}
	\label{tab:proof-trees}
\end{figure}

It is important to note that here the proof trees are done ``in the wrong direction'', as they are written
top-down instead of bottom-up in the sense that we start by the axioms which are the typing judgements of
constants in the lexicon (this could actually be said to be a part of the sentence, but non-determinism in the
meanings is fine as long as the syntactic category is provided in the parsing tree).
This leads to proof trees being a bit weird to the type theorist and a bit weird to the linguist as they are
not written as usual trees.
Moreover, while a proof tree only provides a type, the tree also provides the sequence of function applications
that is needed to compute the actual denotation.

This leads the induced algorithm (for computing the set of denotations) to be exponential in the input in the
worst case, when using the recursive scheme that naturally arises from the definition of $PT$.
Indeed, and while this may seem weird, in the case of a function $\f{F}\ta \to \tb$ where $\f{F}$ is applicative
and the other combinator is of type $\f{F}\ta$, one might apply the function directly or apply it under the $\f{F}$
of the argument, leading to two different proof trees:
\begin{center}
	\begin{align*}
		\resizebox{\textwidth}{!}{$\prooffrac{\cont \f{F}: \mC \Rightarrow \mC \poulpe \Junit{\f{F}}{l}{\ta}{r}{\tb}}{\cont \f{F}lr: \f{F}\tb}$} \\[1em]
		\Japp{l}{\f{F}\ta}{\tb}{r}
	\end{align*}
\end{center}
Those two different proof trees have different semantic interpretations that may be useful, as discussed by
\newcite{bumfordEffectdrivenInterpretationFunctors2025}, especially in their analysis of closure properties, which
is modified in the following section.

\subsubsection{Islands}
\newcite{bumfordEffectdrivenInterpretationFunctors2025} provide a formal analysis of islands\footnote{Syntactic
	structures which prevent some notion of moving outside of the island.} based on a islands being a different type
of branching nodes in the syntactic tree of a sentence, which asks to resolve all $\f{C}$ effects\footnote{Those
	represent continuations.} before that node or being resolved at that node.
The main example they propose is the one of existential closure inside conditioning.
To reconcile this inside our type system, we propose the following change to their formalism: once the syntactic
information of an island existing is added to the tree (or at semantic parsing time, this does not change the
time complexity), we \emph{mark} each node inside the island by adding an ``void effect'' to it, in the same
way as we did for our model of plural.
This translates into a functor which just maintains the \emph{island marker} on \texttt{fmap} and add to the
words which create an island node\footnote{Or \emph{post-compose} the proof trees with a rule on handling the
	$\f{C}$ effect.} a function which handles the $\f{C}$ effects, which could be seen as adding a node in the
semantics parse tree from the syntactic parse tree.
A way to do this would be the following: we pass all functors until finding a $\f{C}$, handle it inside the other
functors and keep going, on both sides, where $\mathrm{PT}$ is defined in Figure \ref{tab:proof-trees}:
\begin{equation*}
	\mathrm{PT}\left(
	\suppfrac{\raisebox{-5pt}{$\cont x_{1}: \f{F}\f{C}\f{F'}\tau$}}{\cont x_{1}: \f{F}\f{F'}\tau}\suchthat
	\suppfrac{\raisebox{-5pt}{$\cont x_{2}: \f{F}\f{C}\f{F'}\tau$}}{\cont x_{2}: \f{F}\f{F'}\tau}
	\right)
\end{equation*}
Note that this preserves the linear size of the parse tree in the number of input words, as we at most double the
number of nodes, and note that this would be preserved with additional similar constructs.

\medskip

This idea amounts to seeing islands, whatever their type, as a form of grammatical/syntactic effect, which is
still part of the language but not of the lexicon, a bit like future, modality or plurals, without the semantic
component.
The idea of seeing everything as effects, while semantically void, allows us to translate into our theory of
type-driven semantics outer information from syntax, morphology or phonology that influences the semantics of the
sentence.
Other examples of this can be found in the modelisation of plural (for morphology, see Sections \ref{par:higherorder}
and \ref{subsec:modality}) and the emphasis of the words by the $\f{F}$ effect (for phonology), and show the
empirical well-foundedness of this point of view.
While we do not aim to provide a theory of everything linguistics through categories\footnote{``\emph{A theory is as
		large as its most well understood examples}''.}, the idea of expressing everything in our effect-driven type-driven
theory of semantics allows us to prepare for any theoretical or empirical observation that has an impact on the
semantics of a word/sentence, the allowed combinators and even the addition of rules.

\subsection{Syntactic-Semantic Parsing}
\label{subsec:ssparsing}
\subsubsection{The Improved Method}
If using a naïve strategy on the trees yields an exponential algorithm,
the best way to improve the algorithm, is to directly leave the naïve strategy.
To do so, we could simply extend the grammar system used to do the syntactic
part of the parsing.
In this section, we will take the example of a CFG since it suffices to create
our typing combinators,
For example, we can increase the alphabet to be the free monoid on the product
$\Sigma$ of the usual base alphabet and $\bar{\mC}$ our typing category.
Since the usual syntactic structure and our combination modes can both be
expressed by CFGs (or almost, cf Figure \ref{fig:combination-cfg}), our product
system can be expressed by (almost) a CFG.
Here, we change our notations from the proof trees of Figure
\ref{tab:proof-trees} to have a more explicit grammar of combination modes
provided below is a rewriting of the proof trees provided earlier, and
highlights the combination modes in a simpler way, based on
\cite{bumfordEffectdrivenInterpretationFunctors2025} as it simplifies the
rewriting of our typing judgements in a CFG\footnote{That, in a sense, was
	already implicitly provided in Figure \ref{tab:judgements}.}.
The grammars provided  in Figures \ref{fig:english-cfg} and \ref{fig:combination-cfg}
are actually used from right to left, as we actually do combinations over the
types and syntactic categories of the words and try to reduce the sentence, not
derive it from nothing.
Now, all the improvements discussed in Section \ref{subsec:rewrite}
can still be applied here, just a bit differently, as this amounts to reducing
ambiguity in the grammar.

\begin{figure}
	\centering
	\input{aux/figures/combination-cfg}
	\caption{Possible Type Combinations in the form of a near CFG}
	\label{fig:combination-cfg}
\end{figure}

This grammar works in five major sections:
\begin{enumerate}
	\item We reintroduce the grammar defining the type and effect system.
	\item We introduce a structure for the semantic parse trees and their labels,
	      the combination modes from
	      \newcite{bumfordEffectdrivenInterpretationFunctors2025}.
	\item We introduce rules for basic type combinations.
	\item We introduce rules for higher-order unary type combinators.
	\item We introduce rules for higher-order binary type combinators.
\end{enumerate}
The idea of the \emph{grammatical} reduction is that from the flat sentence,
we create a full parse tree as a sequence of types $\tau$.
We then reduce it using the binary effect combinators, before choosing the
appropriate binary type combinator.
It is at this point in the reduction we actually do the composition
of functions.
We close up the reduction\footnote{For the combination of two words, which is
	then repeated along the syntactic structure.} by possibly using unary effect
combinators.

\medskip

We do not prove here that these typing rules cannot be written in a simpler
syntactic structure\footnote{It is important to note that for a typing system,
	we only have syntactic-like rules that allow, or not, to combine types.}, but
it is easy to see why a regular expression would not work, and since we need
trees, to express our full system, the best we can do would be to disambiguate
the grammar.
A thing to note is that this grammar is not complete but explains how types can
be combined in a compact form.
For example, the rules of the form $\combML_{\f{F}} M, \tau' \gets M, \tau$ are
rules that provide ways to combine effects from the two inputs in the order we
want to: we can combine $\f{R}\f{S}\e$ and $\f{C}\f{W}(\e \to \t)$ into
$\f{R}\f{C}\f{W}\f{S}\t$ with the mode $\combML\combMR\combMR\combML <$ (see
\cite{bumfordEffectdrivenInterpretationFunctors2025} Example 5.14).

\begin{figure}
	\centering
	\input{aux/figures/combinator-denotations}
	\caption{Denotations describing the effect of the combinators used in the
		grammar describing our combination modes presented in
		Figure \ref{fig:combination-cfg}}
	\label{fig:combinator-denotations}
\end{figure}

Each of these combinators can be, up to order, associated with a inference
rule, and, as such, with a higher-order denotation, which explains the actual
effect\footnote{Pun intended} of the combinator, and are described in
Figure \ref{fig:combinator-denotations}.
The main reason we need to get derivations associated to combinators, is to
properly define equivalence and reduce the number of rules in the grammar.
Explanation on how that is done will come in Section \ref{subsec:rewrite}.
The important thing on those derivation is that they're a direct translation
of the rules defining the notions of functors, applicatives, monads and thus
are not specific to any denotation system, even though we will use
lambda-calculus styled denotations to describe them.
The same structure for combinators apply when describing the combinators
than when describing their rules of existence.

\medskip

Moreover, it is important to note that while we talk about the structure in
Figure \ref{fig:combination-cfg} as a grammar, it is more of a structure that
computes if it is possible or not to combine two types and how.
In particular, our grammar is not finite, since there are infinitely many types
that can be playing the part of $\alpha$ and $\beta$, and infinitely many
functors that can play the roles for $\f{F}$ and so on, but that does not pose
a problem, as recognizing the general form of a type can be done in constant
time for the forms that we want to check, given a proper memory representation.
Furthermore, it can easily be rendered into something that looks like a
Chomsky Normal Form for a CFG and allows us to integrate this in the CYK
algorithm, and even get a correct time complexity.
Since our grammar is not actually finite, the modified version of CYK that
parses only the types (not the full system) will have a complexity in the
size of $\mFunc\left(\mL\right)$ that is linear, albeit with a not so
small constant factor, which comes from the fact that our grammar's size is
linear in $\abs{\mFunc\left(\mL\right)}$.

\begin{thm}
	\label{thm:ptime-parse}
	Semantic parsing of a sentence is polynomial in the length of the	sentence
	and the size of the type system and syntax system.
\end{thm}
\begin{proof}
	Suppose we are given a syntactic generating structure $G_{s}$ along with our
	type combination grammar $G_{\tau}$.
	The system $G$ that can be constructed from the (tensor) product of $G_{s}$ and
	$G_{\tau}$ has size $\abs{G_{s}}\times \abs{G_{\tau}}$.
	Indeed, we think of its rules as a rule for the syntactic categories and a rule
	for the type combinations. Its terminals and non terminals are also in the
	cartesian products of the adequate sets in the original grammars.
	What this in turn means, is that if we can syntactically parse a sentence in
	polynomial time in the size of the input and in $\abs{G_{s}}$, then we can
	syntactico-semantically parse it in polynomial time in the size of the input,
	$\abs{G_{s}}$ and $\abs{G_{\tau}}$.
\end{proof}

While we have gone with the hypothesis that we have a CFG for our language,
any type of polynomial-time structure could work\footnote{Simply, the algorithm
	will need to be	adapted to at least process the CFG for the typing rules.},
as long as it is more expressive than a CFG.
We will do the following analysis using a CFG since it allows to cover enough of
the language for our example and for simplicity of describing the process of
adding the typing CFG, even though some think that English is not a context
free language \cite{higginbothamEnglishNotContextFree1984}.
Since the CYK algorithm provides us with an algorithm for CFG based parsing
cubic in the input and linear in the grammar size, we end up with an algorithm
for semantically parsing a sentence that is cubic in the length of the sentence
and linear in the size of the grammar modeling the language and the type system.

\begin{thm}
	\label{thm:ptime-denot}
	Retrieving a pure denotation for a sentence can be done in polynomial time in
	the length of the sentence, given a polynomial time syntactic parsing
	algorithm and polynomial time combinators.
\end{thm}
\begin{proof}
	We have proved in Theorem \ref{thm:ptime-parse} that we can retrieve a
	semantic parse tree from a	sentence in polynomial time in the input.
	Since we also have shown that the length of a semantic parse tree is quadratic
	in the length of the sentence it represents, being linear in the length of a
	syntactic parse tree linear in the length of the sentence.
	We have already seen that given a denotation, handling all effects and
	reducing effect handling to normal forms can be done in polynomial time.
	The superposition of these steps yields a polynomial-time algorithm in the
	length of the input sentence.
\end{proof}

The \emph{polynomial time combinators} assumption is not a complex assumption,
this is for example true for our denotations in Section \ref{sec:language},
with function application being linear in the number of uses of variables in
the function term, which in turn is linear in the number of terms used to
construct the function term and thus of words, and \fmap being in constant
time\footnote{Depending on the functor but still bounded by a constant.} for
the same reason.
Of course, function application could be said to be in constant time too, but
we consider here the duration of the $\beta$-reduction:
\begin{equation*}
	\left(\lambda x. M\right)N \xrightarrow{\beta} M\left[x / N\right]
\end{equation*}

\subsubsection{Diagrammatical Parsing}
\label{subsubsec:diagram-parsing}
When considering \newcite{coeckeMathematicalFoundationsCompositional2010}
way of using string diagrams for syntactic parsing/reductions, we can see them
as (yet) another way of writing our parsing rules.
In our typed category, we can make see our combinators as natural
transformations ($2$-cells): then we can see the different sets of combinators
as different arity natural transformations.
$>$, $\combML_{\f{F}}$ and $\combJ_{\f{F}}$ are represented in
Figure \ref{fig:combinator-sd}, up to the coloring of the regions, because that
is purely for an artistic rendition\footnote{Colours make this less boring,
	trust me}.

\begin{figure}
	\centering
	\input{aux/figures/combinators-sd}
	\caption{String Diagramatic Representation of Combinator Modes}
	\label{fig:combinator-sd}
\end{figure}

Now, a way to see this would be to think of this as an orthogonal set of
diagrams to the ones of Section \ref{sec:nondet}: we could use the syntactic
version of the diagrams to model our parsing, according to the rules in
Figure \ref{fig:combination-cfg}, and then combine the diagrams as in Equation
\ref{eq:CSD}, as shown in Figure \ref{fig:parsing-diagram}, which highlights
why we can consider the diagrams orthogonals.
The obvious reaction\footnote{That I'm sure everyone have had seeing this.} is:
"\emph{Why did we start by saying we could just paste?}".
This is a perfectly valid point, and pasting diagrams is simply a notation
abuse, that is justified by the fact there is a possible reduction.
In this figure we exactly see the sequence of reductions play out on the types
of the words, and thus we also see what exact \emph{quasi-braiding} would be
needed to construct the effect diagram.
Here we talk about \emph{quasi-braiding} because, in a sense, we use $2$-cells
to do braiding-like\footnote{Permutations on the order of the strings}
operations on the strings, and don't actually allow for braiding inside the
diagrammatic computation.

\begin{figure}
	\centering
	\inputtikz{parsing-diagram}
	\caption{Representation of a parsing (sub-)diagram for the sentence
		\emph{the cat eats a mouse}, including the connection between the effect
		handling diagrams from Section \ref{sec:nondet} and the syntactio-semantic
		parsing diagrams formed combining our approach to parsing and
		\cite{coeckeMathematicalFoundationsCompositional2010} approach to diagrams}
	\label{fig:parsing-diagram}
\end{figure}

Categorically, what this means is that we start from a meaning category $\mC$,
our typing category, and take it as our grammatical category.
This is a form of extension on the monoidal version by
\newcite{coeckeMathematicalFoundationsCompositional2010}, as it is seemingly a
typed version, where we change the Pregroup category for the typing category,
possibly taken with a product for representation of the English grammar
representation, to accomodate for syntactic typing on top of semantic typing.
This is, again, just another rewriting of our typing rules.

More formally, we have a first axis of string diagrams in the category
$\mC$ - our string diagrams for effect handling, as in Section
\ref{sec:nondet} - and a second \emph{orthogonal} axis of string diagrams
on a larger category, with endofunctors\footnote{To ensure proper typing of the
	diagrams.} graded by the types in our typing category $\bar{\mC}$ and
with natural transformations mapping the combinators defined in Figures
\ref{fig:combination-cfg} and \ref{fig:combinator-denotations}.
The category in which we consider the second-axis string diagrams does not have
a meaning in our compositional semantics theory, and to be more precise, we
should talk about $1$-cells and $2$-cells instead of functors and natural
transformations, to keep in the idea that this is really just a diagrammatic
way of computing and presenting the operations that are put to work during
semantic parsing.

\medskip

What the lines leading from combinators to functors\footnote{Remember that
	types are objects in $\mC$ and thus functors from
	$\mathds{1} \Rightarrow \mC$} mean categorically, is void in either category.
Those lines are not actually part of the first axis of the string diagram,
nor are they part of the second axis of the string diagram.
They are used to map out the link between the two: they express the
quasi-braiding step proposed above, and present graphically why the order of
the strings in the resulting diagram is as it is, and what the pasting and
quasi-braiding orders should be.
A good approach to these diagrams might be the following: they are an extension
of the parse trees presented in
\newcite{bumfordEffectdrivenInterpretationFunctors2025}, applied to the
formalism of \newcite{coeckeMathematicalFoundationsCompositional2010} and
extended to be integrated with the handling of effects,
as described in Section \ref{sec:nondet}, forming a full diagrammatic calculus
system for semantic parsing.

\medskip

For the combinators $\combJ$, $\combDN$ and $\combC$, which are applied to
reduce the number of effects inside a denotation, it might seem less obvious
how to include them.
While this may seem true, it's actually only true for the \emph{connecting}
strings.
Indeed, applying them to the actual \emph{parsing} part of the diagram is done
in the exact same way as in the CFG, we just apply them when needed, and they
will appear in the resulting denotation as an end for a string, a form of
forced handling, in a sense, as shown in the result of Figure
\ref{fig:parsing-diagram2}.
For the connecting strings, it's simply a matter of adding a new \emph{phantom}
string that will send the associated $2$-cell in the effect handling diagram to
the connected strings.
In particular it is interesting to note that the resulting diagram representing
the sentence can, in a way, be found in the connection strings that arise from
the combinators.

\begin{figure}
	\centering
	\inputtikz{parsing-diagram2}
	\caption{Example of a parsing (sub-)diagram for the phrase
		\emph{a cat in a box}, presenting the integration of unary combinators
		inside the connector line.}
	\label{fig:parsing-diagram2}
\end{figure}

The main reason why this point of view of diagrammatic parsing is useful
will be clear when looking at the rewriting rules and the normal forms they
induce, because, as seen before, string diagrams make it easy to compute
normal forms, when provided with a confluent reduction system.

\subsubsection{The Coproduct Categorical Approach to Syntax}
\label{subsubsec:coprod}
% TODO: add a reference to Bella's paper explaining the interest of such 
% notions when talking about the syntax-semantic interface/
In this section based on the work by
\newcite{marcollimatildeetchomskynoametberwickrobertc.MathematicalStructureSyntactic},
we explore the integration of our notion inside the structure of syntactic
merge, and why the two formalisms are compatible.
The idea behind that structure is to see the union of trees as a product and
the merging of trees as a coproduct in a well-defined Hopf algebra.
Now of course, with our insights on syntactico-semantic parsing, we can either
think of our parsing structure labeled by multiple modes, as in
\cite{bumfordEffectdrivenInterpretationFunctors2025}, or think of it as string
diagrams, as presented above.
In both cases, we make a more or less implicit use of the merge operation.

Indeed, using trees, we get back to the graded non-associative commutative
magma operation proposed in the coproduct approach.
While using string diagrams, we have a certain set of objects on which we can
define a product - the tensor product of diagrams, that is, the monoidal
operation of the category used to consider the diagrams - and a coproduct.
Defining the coproduct is a bit trickier so we will define it \emph{with our
	hands} in an intuitive manner.
The coproduct on the trees simply connects to the merging of trees, so in our
case it should only connect two different bits of our string diagrams together.
This means that our coproduct should be adding a base type combinator but that
doesn't suffice to fully express our typed semantics.
The question can be thus rephrased as follows: how to get a functorial
correspondance between the labeled trees and our string diagrams that behaves
well with regards to the product and coproduct on those objects?

\medskip

However, since our diagrams are a tool that can be seen \emph{on top} of the
usual parsing trees, answering such complex questions might seem a stretch for
an unaware linguist.
Indeed, what such a point of view would bring is void: our system does not
add any new theoretical concept to the theoretical explanation of the parsing
tree approach, and is simply a more visual explanation of the process, as well
as an explanation of it inside the categorical framework we used to develop the
type and effect system for a natural language.
The interest of such a connection is found in the sheer efficient beauty of a
commutative diagram: we express that - however complicated - the two completely
different point of views we have actually do represent the same concept.
While this is true for trees and coproducts - since we did a rewriting of
semantic combinations as a form of enhanced syntax, previous results stand -
this is not in itself obvious for string diagrams.
When considering basic trees and coproducts, this is not possible, the best
we could have would be a forgetful functor from the string diagrams to the
trees.
We would thus need to consider multi-trees, with possibly multiple edges
between a node and one of its children.


\subsection{Rewriting Rules}
\label{subsec:rewrite}
Here we provide a rewriting system on derivation trees that allows us to
improve our time complexity by reducing the size of the disambigued grammar.
In the worst case, in big o notation there is no improvement in the size of the
sentence, but there is no loss.
The goal is to reduce branching in the definition of $PT$, as this will easily
translate into reductions in the grammar.

\medskip

When considering the grammar version of the semantic system, the reductions
written below cannot be done when using only one step at a time.
To get around this, a simple way might be to expand the size of the grammar to
consider reductions two at a time, by adding an intermediate non-terminal.
Then, while the size of the grammar becomes quadratic in
$\abs{\mFunc\left(\mL\right)}$, because this also reduces the number of
reductions needed to get the derivations, this compensates the increase, and
actually reduces the time complexity when the reducing the number of rules
after equivalence.
Obviously the same reasoning applies when considering reductions of length $3$,
$4$ and more.
We really just need to consider the same set of reductions that the one
proposed below.

\medskip

First, let's consider the reductions proposed in Section \ref{sec:nondet}.
Those define normal forms under Theorem \ref{thm:confluence} and thus we can
use those to reduce the number of trees.
Indeed, we usually want to reduce the number of effects by first using the
rules \emph{inside} the language, understand, the adjunction co-unit and
monadic join.
Thus, when we have a presupposition of the form:
\begin{equation*}
	\PT{\blank}{\cont x: \f{M}\f{M}\tau} \poulpe \text{or} \poulpe
	\PT{\blank}{\cont x: RL\tau}
\end{equation*}
we always simplify it directly, as not always doing it would propagate
multiple trees.
This in turn means we always verify the derivational rules we set up in the
previous section for the join equations of the monad.

\medskip

Secondly, while there is no way to reduce the branching proposed in the
previous section since they end in completely different reductions, there is
another case in which two different reductions arise:
\begin{equation*}
	\PT{\blank}{\cont x: \f{F}\tau_{1}} \poulpe \text{and} \poulpe
	\PT{\blank}{\cont y: \f{G}\tau_{2}}
\end{equation*}
Indeed, in that case we could either get a result with effects $\f{F}\f{G}$ or
with effects $\f{G}\f{F}$.
In general those effects are not the same, but if they happen to be, which
trivially will be the case when one of the effects is external, the plural or
islands functors for example.
When the two functors commute, the order of application does not matter and
thus we choose to get the outer effect the one of the left side of the
combinator.

\medskip

Thirdly, there are modes that clearly encompass other ones\footnote{Here we use
	the grammar notation for ease of explanation}.
One should not use mode $\combUR$ when using $\combMR$ or $\combDN \combMR$
and the same goes for the left side, because the two derivations yield simpler
results.
Same things can be said for certain other derivations containing the lowering
and co-unit combinators.

\noindent We use $\combDN$ when we have not used any of the following, in all
derivations:
\begin{multicols}{2}
	\begin{itemize}
		\item $m_{\f{F}}, \combDN, m_{\f{F}}$ where
		      $m \in \{\combMR, \combML\}$
		\item $\combML_{\f{F}}, \combDN, \combMR_{\f{F}}$
		\item $\combA_{\f{F}}, \combDN, \combMR_{\f{F}}$
		\item $\combML_{\f{F}}, \combDN, \combA_{\f{F}}$
		\item $\combC$
	\end{itemize}
\end{multicols}
\noindent We use $\combJ$ if we have not used any of the following,
for $j \in \{\epsilon, \combJ_{\f{F}}\}$
\begin{multicols}{2}
	\begin{itemize}
		\item $\left\{m_{\f{F}}, j, m_{\f{F}}\right\}$ where
		      $m \in \{\combMR, \combML\}$
		\item $\combML_{\f{F}}, j, \combMR_{\f{f}}$
		\item $\combA_{\f{F}}, j, \combMR_{\f{F}}$,
		\item $\combML_{\f{F}}, j, \combA_{\f{F}}$
		\item $k, \combC$ for $k \in \{\epsilon, \combA_{\f{F}}\}$
		\item If $\f{F}$ is commutative as a monad:
		      \begin{itemize}
			      \item $\combMR_{\f{F}}, \combA_{\f{F}}$
			      \item $\combA_{\f{F}}, \combML_{\f{F}}$
			      \item $\combMR_{\f{F}}, j, \combML_{\f{F}}$
			      \item $\combA_{\f{F}}, j, \combA_{\f{F}}$
		      \end{itemize}
	\end{itemize}
\end{multicols}

\begin{thm}
	The rules proposed above yield equivalent results.
\end{thm}
\begin{proof}
	For the first point, the equivalence has already been proved under Theorem
	\ref{thm:confluence}.
	For the second point, it is obvious since based on an equality.
	To prove the third set of rules, we have used the proof assistant Lean.
	We have provided it with a full set of tactics about $\lambda$-calculus,
	to allow to check equivalence of terms and added definitions for our types,
	combinators and so on, to make use of its language server\footnote{Lean was
		also used by Dylan \textsc{Bumford} to rewrite the full parser and make use
		of its dependent type system, allowing for functions about types and
		effects to be typed.}
\end{proof}

\medskip

The observant reader might have noticed that this is simply a scheme to apply
typing rules to a syntactic derivation, but that this will not be enough to
actually gain all reductions possible in polynomial time.
This is actually not feasible (because of the intrinsic ambiguity of the
English language, as proved for example by the sentence \emph{The man sees the
	girl using a telescope}).
Even then, we are far from reducing to a minimum the number of different paths
possible to get a same final denotation.
This can only happen once a confluent reduction scheme is provided for the
denotations.
When this is done, we can combine the reduction schemes for effects along with
the one for denotation and the one for combinations in one large reduction
scheme.
Indeed, trivally, the tensor product of confluent reduction schemes forms a
confluent reduction scheme, whose maximal reduction length is the sum of the
maximal reduction lengths\footnote{Actually it is at most that, but that does
	not	really matter.}.

\medskip

When using our diagrammatic approach to parsing, which, again, is just a
rewriting in a more graphical fashion of our typing rules and our CFG-like
rules, we can write all the reductions described above to our paradigm:
it simply amounts to constructing a set of normal forms for the string diagrams.
This leads to the same algorithms developed in Section \ref{sec:nondet} being
usable here: we just have a new improved version of Theorem
\ref{thm:confluence} which adds the normal forms specified in this section to
the newly added \emph{orthogonal} axis of diagrammatic computations.
What we're actually doing is computing two different normal forms along the
tensor product of our reduction schemes\footnote{Just like we already have (or
	should have) a tensor scheme for the denotations and combination modes.},
but again, that only amounts to computing a larger normal form.
Moreover, considering the possible normal forms of syntactic reductions simply
adds another way to reduce our diagrams to normal forms.
Since all of these forms can be attained in polynomial time, it is clear that
finding a normal-form diagram, which is exactly a normal-form denotation for
the sentence, is doable in polynomial time\footnote{Of course, this is only
	true when the denotations can be normalized in polynomial time.}

\medskip

Once again, there is no evidence that our system is complete, if not the
contrary, so the arguments developed in Section \ref{subsubsec:sanity} are
still valid.
A way to "complete" it, although it would still probably be incomplete would be
to write an automatized prover in Lean, but this is out of the scope of this
project, as it would not do many improvements.
