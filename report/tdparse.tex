\documentclass[math, english, info]{cours}

\makeatletter
\def\tikzimp@rt{0}
\makeatother

\input{preamble}

\title{Formalizing Typing Rules for Natural Languages using Effects}
\author{Matthieu Boyer}

\begin{document}
\bettertitle

\begin{abstract}
	The main idea is that you can consider some words to act as functors in a typing category, for example determiners:
	they don't change the way a word acts in a sentence,
	but more the setting in which the word works by adding an effect to the work.
	This document is based mostly on \newcite{bumfordEffectdrivenInterpretation} and \newcite{bumfordEffectfulCompositionNatural}.
\end{abstract}

\section*{Introduction}
\newcite{moggiComputationalLambdacalculusMonads1989} provided a way to view monads as effects
in functional programming. This allows for new modes of combination in a compositional
semantic formalism, and provides a way to model words which usually raise problems with the
traditional lambda-calculus representation of the words. In particular we consider words such
as \textsl{the} or \textsl{a} whose application to common nouns results in types that should
be used in similar situations but with largely different semantics, and model those as
functions whose application yields an effect. This allows us to develop typing judgements and
an extended typing system for compositional semantics of natural languages.
Type-driven compositional semantics acts under the premise that given a set of words and their
denotations, a set of grammar rules for composition and their associated typing judgements,
we are able to form an enhanced parser for natural language which provides a mathematical
representation of the meaning of sentences, as proposed by \newcite{heimSemanticsGenerativeGrammar1998}.

\medskip

This is not the first time a categorical representation of a compositional semantics of
natural language is proposed, \newcite{coeckeMathematicalFoundationsCompositional2010}
already suggested an approach based on monoidal categories using an outside model of meaning.
However, what we propose here is a representation of the different capabilities of words
as categorical constructs: we allow for a wider set of representations inside our
model of meaning, trading non-determinism for additional structures.
In this regard, we basically combine the grammatical type and the meaning of a word
by having our denotations be associated with a type: there is no need from an additional
category outside of our typing category and we limit ourselves to reducing non-determinism
by limiting our possibilities for combinations with a provided CFG\footnote{Or another model
	that could generate our language.} of the language.

The focus of our system is to allow more flexibility in denotations, leading to more
possibilities of combinations.

\section{Categorical Semantics of Effects: A Typing System}
\label{sec:typingsystem}
In this section, we will designate by $\mL$ our language, as a set of words (with their semantics) and syntactic rules to combine them semantically.
We denote by $\O\left( \mL \right)$ the set of words in the language whose semantic representation is a low-order function and $\mF\left( \mL \right)$ the set of words whose semantic representation is a functor or high-order function.
Our goals here are to describe more formally, using a categorical vocabulary, the environment in which the typing system for our language will exist, and how we connect words and other linguistic objects to the categorical formulation.

\subsection{Typing Category}\label{subsec:typingcategory}
\subsubsection{Types}\label{subsubsec:types}
Let $\mC$ be a closed cartesian category. This represents our main typing system, consisting of words $\O(\mL)$ that can be expressed without effects.
Remember that $\mC$ contains a terminal object $\bot$ representing the empty type or the lack thereof.
We can consider $\bar{\mC}$ the category closure of $\mF\left( \mL \right)\left(\O\left( \mL \right)\right)$, that is consisting of all the different type constructors (ergo, functors) that could be formed in the language.
What this means is that we consider for our category objects any object that can be attained in a finite number from a finite number of functorial applications from an object of $\mC$.
In that sense, $\bar{\mC}$ is still a closed cartesian category (since all our functors induce isomorphisms on their image)\footnote{Our definition does not yield a closed cartesian category as there are no exponential objects between results of two different functors, but this is not a real issue as we can just say we add those.}.
$\bar{\mC}$ will be our typing category (in a way).

We consider for our types the quotient set $\star = \mathrm{Obj}\left( \bar{\mC} \right)/\mF\left( \mL \right)$.
Since $\mF\left( \mL \right)$ does not induce an equivalence relation on $\Obj\left( \bar{\mC} \right)$ but a preorder, we consider the chains obtained by the closure of the relation $x\succeq y \Leftrightarrow \exists F, y = F(x)$ (which is seen as a subtyping relation as proposed in \newcite{melliesFunctorsAreType2015}).
We also define $\star_{0}$ to be the set obtained when considering types which have not yet been \emph{affected}, that is $\Obj(\mC)$.
In contexts of polymorphism, we identify $\star_{0}$ to the adequate subset of $\star$.
In this paradigm, constant objects (or results of fully done computations) are functions with type $\bot \to \tau$ which we will denote directly by $\tau \in \star_{0}$.

What this construct actually means, in categorical terms, is that a type is an object of $\bar{\mC}$ (as intended) but we add a subtyping relationship based on the procedure used to construct $\bar{\mC}$.
Note that we can translate that subtyping relationship on functions as $F\left( A \xrightarrow{\phi} B \right)$ has types $F\left( A\Rightarrow B \right)$ and $FA \Rightarrow FB$.

We will provide in Table \ref{tab:sctypes} a list of the effect-less usual types associated to regular linguistic objects.

\subsubsection{Functors, Applicatives and Monads}\label{subsubsec:functors}
Our point of view leads us to consider \emph{language functors}\footnote{The elements of our language, not the categorical construct.} as polymorphic functions: for a (possibly restrained, though it seems to always be $\star$) set of base types $S$, a functor is a function
\begin{equation*}
	x: \tau\in S\subseteq \star \mapsto F x: F\tau
\end{equation*}
Remember that $\star$ is a fibration of the types in $\bar{\mC}$.
This means that if a functor can be applied to a type, it can also be applied to all \emph{affected} versions of that type, i.e. $\mF\left( L \right)(\tau\in \star)$.
More importantly, while it seems that $F$'s type is the identity on $\star$, the important part is that it changes the effects applied to $x$ (or $\tau$).
In that sense, $F$ has the following typing judgements:
\begin{equation*}
	\frac{\Gamma\vdash x: \tau \in \star_{0}}{\Gamma\vdash F x: F\tau \notin \star_{0}}\fracnotate{$\text{Func}_{0}$} \hspace{2cm} \frac{\Gamma\vdash x: \tau}{\Gamma\vdash Fx : F\tau\preceq \tau}\fracnotate{Func}
\end{equation*}
We use the same notation for the \emph{language functor} and the \emph{type functor} in the examples, but it is important to note those are two different objects, although connected.
More precisely, the \emph{language functor} is to be seen as a function whose computation yields an effect, while the \emph{type functor} is the endofunctor of $\bar{\mC}$ (so a functor from $\mC$) that represents the effect in our typing category.

In the same fashion, we can consider functions to have a type in $\star$ or more precisely of the form $\star \to \star$ which is a subset of $\star$.
This justifies how functors can act on functions in our typing system, thanks to the subtyping judgement introduced above, as this provides a way to ensure proper typing while just propagating the effects.
Because of propagation, this also means we can resolve the effects or keep on with the computation at any point during parsing, without any fear that the results may differ.

\medskip

In that sense, applicatives and monads only provide with more flexibility on the ways to combine functions:
they provide intermediate judgements to help with the combination of trees.
For example, the multiplication of the monad provides a new \emph{type conversion} judgement:
\begin{equation*}
	\frac{\Gamma\vdash x: MM\tau}{\Gamma\vdash x: M\tau \succeq MM\tau}\fracnotate{Monad}
\end{equation*}
This is actually a special case of the natural transformation rule that we define below, which means that, in a way, types $MM\star$ and $M\star$ are equivalent, as there is a canonical way to go from one type to another.
Remember however that $M\star$ is still a proper subtype of $MM\star$ and that the objects are not actually equal:
they are simply equivalent.

\subsubsection{Natural Transformations for Handlers and Higher-Order Constructs}\label{subsubsec:transnat}
We could also add judgements for adjunctions, but the most interesting thing is to add judgements for natural transformations, as adjunctions are particular examples of natural transformations which arise from \emph{natural} settings.
While in general we do not want to find natural transformations, we want to be able to express these in three situations:
\begin{enumerate}
	\item If we have an adjunction $L\dashv R$, we have natural transformations for $\Id_{\mC} \Rightarrow L \circ R$ and $R\circ L \Rightarrow \Id_{\mC}$.
	      In particular we get a monad and a comonad from a canonical setting.
	\item To deal with the resolution of effects, we can map handlers to natural transformations which go from some functor $F$ to the $\Id$ functor, allowing for a sequential\footnote{In particular, non-necessarily commutative} computation of the effects added to the meaning.
		We will develop a bit more on this idea in Paragraph \ref{par:handlers} and in Section \ref{sec:nondet}.
	\item To create \emph{higher-order} constructs which transform words from our language into other words, while keeping the functorial aspect.
	      This idea is developed in \ref{par:higherorder}.
\end{enumerate}
To see why we want this rule, which is a larger version of the monad multiplication and the monad/applicative unit, it suffices to see that the diagram below provides a way to construct the ``correct function'' on the ``correct functor'' side of types. If we have a natural transformation \begin{tikzcd}
	F\ar[r, Rightarrow, "\theta"] & G
\end{tikzcd} then for all arrows $f: \tau_{1} \to \tau_{2}$ we have:
\begin{category}
	F\tau_{1}\ar[r, "Ff"]\ar[d, "\theta_{\tau_{1}}"'] & F\tau_{2}\ar[d, "\theta_{\tau_{2}}"]\\
	G\tau_{1}\ar[r, "Gf"] & G\tau_{2}
\end{category}
and this implies, from it being true for all arrows, that from $\cont x: F\tau$ we have an easy construct to show $\cont x: G\tau$.

Remember that in the Haskell programming language, any polymorphic function is a natural transformation from the first type constructor to the second type constructor, as proved by \newcite{wadlerTheoremsFree1989}.
This will guarantee for us that given a \emph{Haskell} construction for a polymorphic function, we will get the associated natural transformation.

\paragraph{Adjunctions}
\label{par:adjunctions}
We will not go in much details about adjunctions, as a full example and generalization process is provided in \ref{subsec:effects}.
First, we remind the definition: an adjunction $L \dashv R$ is a pair of functors $L: \A \to \B$ and $R: \B \to \A$, and a pair of natural transformations $\eta: \Id_{\A}  \Rightarrow R \circ L$ and $\epsilon: L\circ R \Rightarrow \Id_{\A}$ such that the two following equations are satisfied:
\begin{equation}
	\inputtikz{cd-zigzag}
	\label{eq:zigzag}
	\tag{\emoji{cloud-with-lightning}}
\end{equation}
\begin{equation}
	\inputtikz{cd-zagzig}
	\label{eq:zagzig}
	\tag{\reflectbox{\emoji{cloud-with-lightning}}}
\end{equation}

An adjunction defines two different structures over itself: a monad $L \circ R$ and a comonad $R\circ L$.
The fact these structures arise from the interaction between two effects renders them an intrinsic property of the language.
In this lies the usefulness of adjunction in a typing system which uses effects: adjunctions provide a way to combine effects and to handle effects, allowing to simplify the computations on the free monoid on the set of functors.

\paragraph{Handlers}
\label{par:handlers}
As introduce by \newcite{marsikAlgebraicEffectsHandlers}, the use of handlers as annotations to the syntactic tree of the sentence is an appropriate formalism.
This could also give us a way to construct handlers for our effects as per \newcite{bauerEffectSystemAlgebraic2014}, or \newcite{plotkinHandlingAlgebraicEffects2013}.
As considered by \newcite{wuEffectHandlersScope2014} and \newcite{vandenbergFrameworkHigherorderEffects2024}, handlers are to be seen as natural transformations describing the free monad on an algebraic effect.
Considering handlers as so, allows us to directly handle our computations inside our typing system, by ``transporting'' our functors one order higher up without loss of information or generality since all our functors undergo the same transformation.
Using the framework proposed in \cite{vandenbergFrameworkHigherorderEffects2024} we simply need to create handlers for our effects/functors and we will then have in our language the result needed.

\medskip

What does this mean in our typing category ?
It means that either our language or our parser for the language \footnote{Depending whether we think handling effects is an intrinsic construct of the semantics of the language or whether it is associated with a speaker.} should contain natural transformations $F \Rightarrow \Id$ for $F \in \mF\left( \mL \right)$.
In this goal, we remember that from any polymorphic function in \emph{Haskell} we get a natural transformation \cite{wadlerTheoremsFree1989} meaning that it is enough to be able to define our handler in \emph{Haskell} to be ensured of its good definition.
Note that the choice of the handler being part of the lexicon or the parser over the other is a philosophical question more than a semantical one, as both options will result in semantically equivalent models, the only difference will be in the way we consider the resolution of effects.
Mathematically\footnote{Computer-wise, actually.}, this means the choice of either one of the options is purely of detail left during the implementation.

However, this choice does not arise in the case of the adjunction-induced handlers. Indeed here, the choice is caused by the non-uniqueness of the choices for the handlers.
For example, two different speakers may have different ways to resolve the $\f{S}$ (which will be introduced as the \emph{Powerset} monad in Section \ref{subsec:effects}) that arises from the phrase \textsl{A chair}.
This usual example of the differences between the cognitive representation of words is actually a specific example of the different possible handlers for the powerset representation of non-determinism/indefinites:
there are $\abs{S}$ arrows from the initial object to $S$ in $\mathit{Set}$, representing the different elements of $S$.
In that sense, while handlers may have a normal form or representation purely dependant on the effect, the actual handler does not necessarily have a canonical form.
This is the difference with the adjunctions: adjunctions are intrinsic properties of the coexistence of the effects\footnote{Which we identify to their functorial representations, which we may identify to their free monad in the framework \cite{vandenbergFrameworkHigherorderEffects2024}.}, while the handlers are user-defined.
As such, we choose to say that our handlers are implemented parser-side but again, this does not change our modelisation of handlers as natural transformations and most importantly, this does not add non-determinism to our model:
The non-determinism that arises from the variety of possible handlers does not add to the non-determinism in the parsing.

\paragraph{Higher-Order Constructs}
\label{par:higherorder}
We might want to add plurals, superlatives, tenses, aspects and other similar constructs which act as function modifiers.
For each of these, we give a functor $\Pi$ corresponding to a new class of types along with natural transformations for all other functors $F$ which allows to propagate down the high-order effect.
This transformation will need to be from $\Pi \circ F$ to $\Pi \circ F \circ \Pi$ or simply $\Pi \circ F \Rightarrow F \circ \Pi$ depending on the situation.
This allows us to add complexity not in the compositional aspects but in the lexicon aspects.
We do not want these functors to be applicatives, as we do not want a way to \emph{create} them in the parser if they are not marked in the sentence.

One of the main issues with this is the following:
In the English language, plural is marked on all words (except verbs, and even then case could be made for it to be marked),
while future is marked only on verbs (through the \textit{will + verb} construct which creates a ``\emph{new}'' verb in a sense) though it applies also to the arguments on the verb.
A way to solve this would be to include in the natural transformations rules to propagate the functor depending on the type of the object.
Consider the superlative effect \textbf{most}\footnote{We do not care about morphological markings here, we say that $\mathbf{largest} = \mathbf{most} \left(\mathbf{large}\right)$}.
As it can only be applied on adjectives, we can assume its argument is a function (but the definition would hold anyway taking $\tau_{1} = \bot$).
It is associated with the following function (which is a rewriting of the natural transformation rule):
\begin{equation*}
	\frac{\cont x: \tau_{1} \to \tau_{2}}{\cont \mathbf{most}\, x \coloneqq \Pi_{\tau_{2}} \circ x = x \circ \Pi_{\tau_{1}}}
\end{equation*}
What curryfication implies, is that higher-order constructs can be passed down to the arguments of the functions they are applied to, explaining how we can reconciliate the following semantic equation even if some of the words are not marked properly:
\begin{equation*}
	\bf future\left(be\left( I, a\ cat \right)\right) = will\ be\left( future\left( I \right), a\ cat \right) = be\left( future\left( I \right), a\ cat \right)
\end{equation*}
Indeed the above equation could be simply written by our natural transformation rule as:
\begin{equation*}
	\bf future\left( be \right)\left( arg_{1}, arg_{2} \right) = future\left( be \right)\left( arg_{2} \right)\left( future\left( arg_{1} \right) \right) = future \left( be \right) \left( future \left( arg_{2} \right) \right) \left( future \left( arg_{1} \right) \right)
\end{equation*}
This is not a definitive rule as we could want to stop at a step in the derivation, depending on our understanding of the notion of future in the language.

\subsection{Typing Judgements}\label{subsec:judgements}
To complete this section, Table \ref{tab:judgements} gives a simple list of different typing composition judgements through which we also re-derzive the subtyping judgement to allow for its implementation.
\begin{table}
	\inputtikz{typing-judgements}
	\caption{Typing and Subtyping Judgements}
	\label{tab:judgements}
\end{table}
Note that here, the syntax is not taken into account: a function is always written left of its arguments, whether or not they are actually in that order in the sentence.
This issue will be resolved by giving the syntactic tree of the sentence (or infering it at runtime).
We could also add symmetry induced rules for application.

Furthermore, note that the App rule is the \texttt{fmap} rule for the identity functor and that the \texttt{pure/return} and \texttt{>>=} is the \texttt{nat} rule for the monad unit (or applicative unit) and multiplication.

\medskip

Using these typing rules for our combinators, it is important to see that our grammar will still be ambiguous and thus our reduction process will be non-deterministic.
As an example, we provide the typing reductions for the classic example: \textsl{The man sees the girl using a telescope} in Figure \ref{fig:ud}.

\begin{figure*}
	\centering
	\inputtikz{parse-tree-ex}
	\caption{Parsing trees for the typing of \textsl{The man sees the girl using a telescope}.}
	\label{fig:ud}
\end{figure*}

This non-determinism is a component of our language's grammar and semantics: a same sentence can have multiple interpretation without context.
By the same reasoning we applied on handlers, we choose not to resolve the ambiguity in the parser but we will try in Section \ref{sec:nondet} to reduce it to a minimum.
Even so some of our derivations will include ways to read from a context (for example the \textbf{Jupiter, a planet} construct), we forget about the definition of the context, or its updating after a sentence, in our first proposition.

\section{Language Translation}
\label{sec:language}
In this section we will give a list of words along with a way to express them as either arrows or endo-functors of our typing category.
This will also give a set of functors and constructs in our language.

\subsection{Types of Syntax}\label{subsec:syntax}
We first need to setup some guidelines for our denotations, by asking ourselves how the different components of sentences interact with each other.
For syntactic categories, the types that are generally used are the following, and we will see it matches with our lexicon, and simplifies our functorial definitions\footnote{We don't consider effects in the given typings.}.
Those are based on \newcite{parteeLecture2Lambda}. Here $\Upsilon$ is the operator which retrieves the type from a certain syntactic category.
\begin{table}
	\centering
	\inputtikz{syntactic-categories}
	\caption{Usual Typings for some Syntactic Categories}
	\label{tab:sctypes}
\end{table}
\subsection{Lexicon: Semantic Denotations for Words}\label{subsec:lexicon}
Many words will have basically the same ``high-level'' denotation.
For example, the denotation for most common nouns will be of the form: $\cont \lambda x. \mathbf{planet} x: \e \to \t$.
In Table \ref{tab:lexicon} we give a lexicon for a subset of the english language.
We describe the constructor for the functors used by our denotations in the table, but all functors will be reminded and properly defined in Table \ref{tab:functors} along with their respective \fmap.
\begin{table}
	\centering
	\inputtikz{lexicon-table}
	\caption{$\lambda$-calculus representation of the english language $\mL$}
	\label{tab:lexicon}
\end{table}

\subsection{Effects of the Language}\label{subsec:effects}
For the applicatives/monads in Table \ref{tab:functors} we do not specify the unit and multiplication functions, as they are quite usual examples.
We still provide the \fmap{} for good measure.

\begin{table}
	\centering
	\inputtikz{functors-table}
	\caption{Denotations for the functors used}
	\label{tab:functors}
\end{table}

Let us explain a few of those functors: $\f G$ designates reading from a certain environment of type $\r$ while $\f W$ encodes the possibility of logging a message of type $\t$ along with the expression.
The functor $\f M$ describes the possibility for a computation to fail, for example when retrieving a value that does not exist (see $\mathbf{the}$).
The $\f S$ functor represents the space of possibilities that arises from a non-deterministic computation (see $\mathbf{which}$).

We can then define an adjunction between $\f G$ and $\f W$ using our definitions:
\begin{equation*}
	\phi: \begin{array}{l}
		\left( \alpha \to \f{G}\beta \right)\to \f{W}\alpha \to \beta \\
		\lambda k. \lambda \left( a, g \right) . k a g
	\end{array}
	\text{ and }
	\psi: \begin{array}{l}
		\left( \f{W}\alpha \to \beta \right) \to \alpha \to \f{G} \beta \\
		\lambda c \lambda a \lambda g. c \left( a, g \right)
	\end{array}
\end{equation*}
where $\phi \circ \psi = \id$ and $\psi \circ \phi = \id$ on the properly defined sets.
Now it is easy to see that $\phi$ and $\psi$ do define a natural transformation and thus our adjunction $\f{W} \dashv \f{G}$ is well-defined, and that its unit $\eta$ is $\psi \circ \id$ and its co-unit $\epsilon$ is $\phi \circ \id$.

As a reminder, this means our language canonically defines a monad $\f{G}\circ \f{W}$.
Moreover, the co-unit of the adjunction provides a canonical way for us to deconstruct entirely the comonad $\f{W}\circ \f{G} $, that is we have a natural transformation from $\f{W} \circ \f{G} \Rightarrow \Id$.

\subsection{Modality, Plurality, Aspect, Tense}\label{subsec:modality}
We will now explain ways to formalise higher-order concepts as described in Section \ref{par:higherorder}.

First, let us consider in this example the plural concept.
Here we do not focus on whether our denotation of the plural is semantically correct, but simply on whether our modelisation makes sense.
As such, we give ourselves a way to measure if a certain $x$ is plural our not, which we will denote by $\abs{x} \geq 2$\footnote{We chose this representation as an example, it is not important for our formalism that this denotation is actually a good choice for the plural.}.
It is important to note that if $\tau$ is a type, and $\cont x: \tau$ then we should have $\abs{\Pi x} \geq 1 = \top\footnote{This might be seen as a typing judgement~!}$.
We then need to define the functor $\Pi$ which models the plural. We do not need to define it on types in any way other than $\Pi\left( \tau \right) = \Pi\tau$.
We only need to look at the transformation on arrows\footnote{\texttt{fmap}, basically.}.
This one depends on the \emph{syntactic}\footnote{Actually it depends on the word considered, but since the denotations (when considered effect-less) provided in Table \ref{tab:lexicon} are more or less related to the syntactic category of the word, our approximation suffices.} type of the arrow, as seen in Table \ref{tab:lexicon}.
\begin{table}
	\centering
	\inputtikz{plural-table}
	\caption{(Partial) Definition for the $\Pi$ Plural Functor}
	\label{tab:pluralfunctor}
\end{table}
There is actually a presupposition in our definition. Whenever we apply $\Pi$ to an arrow representing a predicate $p: \e \to \t$, we still apply $p$ to its argument $x$ even though we say that $\Pi p$ is the one that applies to a plural entity $x$.
This slight notation abuse results from our point of view on plural/its predicate representation: we assume the predicate $p$ (the common noun, the adjectve, the VP\ldots) applies to an object without regarding its cardinality.
The two point of views on the singular/plural distinction could be adapted to our formalism: whether we believe that singular is the \emph{natural} state of the objects or that it is to be always specified in the same way as plural does not change anything:
In the former, we do not change anything from the proposed functors.
In the latter we simply need to create another functor $\Sigma$ with basically the same rules that represents the singular and ask of our predicates $p$ to be defined on $\left(\Pi\star\right) \sqcup \left(\Sigma \star\right)$.

\medskip

See that our functor only acts on the words which return a boolean by adding a plurality condition, and is simply passed down on the other words.
Note however there is an issue with the \textbf{NP} category: in the case where a \textbf{NP} is constructed from a determiner and a common noun (phrase) the plural is not passed down.
This comes from the fact that in this case, either the determiner or the common noun (phrase) will be marked, and by functoriality the plural will go up the tree.

Now clearly our functor verifies the identity and composition laws and works as theorised in Section \ref{par:higherorder}.
To understand a bit more why this works as described in our natural transformation rules, consider $\Pi\left( \mathbf{which} \right)$.
The result will be of type $\Pi\left( \f{S}\left( e \right) \right) = \left(\Pi \circ \f{S}\right) \left( e \right)$.
However, when looking at our transformation rule (and our functorial rule) we see that the result will actually be of type $\f{S}\left( \Pi e \right)$ which is exactly the expected type when considering the phrase $\w{which} p$.
The natural transformation $\theta$ we set is easily inferable:
\begin{equation*}
	\theta_{A}:
	\begin{tikzcd}
		\left( \Pi \circ \f{S} \right) A \ar[r, "\theta_{A}"] & \left( \f{S} \circ \Pi \right) A\\[-.7cm]
		\Pi\left( \left\{ x \right\} \right) \ar[r, mapsto] &  \left\{ \Pi(x) \right\}
	\end{tikzcd}
\end{equation*}
The naturality follows from the definition of $\fmap$ for the $\f{S}$ functor.
We can easily define natural transformations for the other functors in a similar way: $\Pi \circ \f{F} \overset{\theta}{\Rightarrow} \f{F} \circ \Pi$ defined by $\theta_{\tau} = \fmap_{\f{F}}\left( \Pi \right)$.

\medskip

Now, in quite a similar way, we can create functors from any sort of judgement that can be seen as a function $\e \to \t$ in our language\footnote{An easy way to define those would be, similarly to the plural, to define a series of judgement from a property that could be infered.}.
Indeed, we simply need to replace the $\abs{p} \geq 1$ in most of the definitions by our function and the rest would stay the same.
Remember that our use of natural transformations is only there to allow for possible under-markings of the considered effects and propagating the value resulting from the computation of the high-order effect.

\section{Solving Non-Determinism}\label{sec:nondet}
The typing judgements proposed in Section \ref{subsec:judgements} lead to ambiguity.
In this section we propose ways to get our derivations to a certain normal form, by deriving an equivalence relation on our derivation and parsing trees, based on string diagrams.

\subsection{String Diagram Modelisation of Sentences}
\label{subsec:sd}
String diagrams are the Poincaré duals of the usual categorical diagrams when considered in the $2$-category of categories and functors.
In this category, the endofunctors are natural transformations.
This means that we represent categories as regions of the plane, functors as lines separating regions and natural transformations as the intersection points between two lines.
We will change the color of the regions when crossing a functor, and draw the identity functor as a dashed line or no line at all in the following sections.
We will always consider application as applying to the right of the line so that composition is written in the same way as in equations:
\begin{center}
	\inputtikz{sd-examples}
\end{center}
This gives us a new graphical formalism to represent our effects using a few equality rules between diagrams.
The commutative aspect of functional diagrams is now replaced by an equality of string diagrams, which will be detailed in the following section.

The important aspect of string diagrams that we will use is that two diagrams that are planarily homotopic are equal \cite{delpeuchNormalizationPlanarString2022} and that we can map a string diagram to a sequence of computations on computation: the vertical composition of natural transformations (bottom-up) represents the reductions that we can do on our set of effects.
This means that even when adding handlers, we get a way to visually see the meaning get reduced from effectful composition to propositional values, without the need to specify what the handler does.
Indeed we only look at \emph{when} we apply handlers and natural transformations reducing the number of effects.
This delimits our usage of string diagrams as ways to look at computations and a tool to provide equality rules to reduce non-determinism by constructing an equivalence relationship (which we denote by $\eqcirc$) and yielding a quotient set of \emph{normal forms} for our computations.

\medskip

First let's look at the fact we can see functors as natural transformations and morphisms as functors, to justify that our typing tree are, in a sense string diagrams.
Let us define the category $\mathds{1}$ with exactly one object and one arrow: the identity on that object. It will be shown in grey in the string diagrams below.
A functor of type $\mathds{1} \to \mC$ is equivalent to choosing an object in $\mC$, and a natural between two such functors $\tau_{1}, \tau_{2}$ is exactly an arrow in $\mC$ of type $\tau_{1} \to \tau_{2}$.
This gives us a way to map the composition of our sentence to a simple string in our diagram.
Knowing that allows us to represent the type resulting from a sequence of computations as a sequence of strings whose farthest right represents an object in $\mC$, that is, a base type.
\begin{center}
	\inputtikz{sd-thecatsleeps}
\end{center}

For simplicity reasons, and because the effects that are buried in our typing system not only give rise to functors but also have types that are not purely currifiable, we will write our string diagrams on the fully parsed sentence, with its most simplified/composed expression.
Indeed, the question of providing rules to compose the string diagram for \textbf{the} and the one for \textbf{cat} to give the one for \textbf{the cat} is a difficult question, that natural solutions to are not obvious\footnote{I will suggest a solution when discussing the set of equations between our diagrams and the way we construct our \emph{normal forms}.}.
The natural composition rules on diagrams being the gluing together of diagrams, we do need to add something to our toolbox.
\begin{equation}
	\inputtikz{sd-csd-example}
	\label{eq:CSD}
	\tag{CSD}
\end{equation}

To justify our proposition to only consider fully reduced expressions note that in this formalism we don't consider the expressions for our functors and natural transformations\footnote{Here, we are talking about the actual \emph{natural transformations} that are used for handling effects, whether they're the ones associated to algebraic handlers or adjunctions/monads\ldots} but simply the sequence in which they are applied.
In particular, this means the following diagrams commute for any $F, G$ functors and $\theta$ natural transformation.
\begin{category}
	G\ar[r, "F"]\ar[d, "\theta"'] & F\circ G\ar[d, "F\circ \theta"] &[2cm] G\ar[r, "F"]\ar[d, "\theta"'] & G\circ F \ar[d, "\theta\circ F"] \\
	\theta G\ar[r, "F"'] & F\circ \theta G & \theta G\ar[r, "F"'] & \theta G \circ F
\end{category}
The property of natural transformations to be applied before or after any arrow in the category justifies that even when composing before the handling we get the same result.
These properties justifies the fact that we only need to prove the equality of two reductions at the farthest step of the reductions, even though in practice the handling might be done at earlier points in the parsing.
We have indeed covered all the composition options of our language by talking about functors and arrows.

In the end, we will have the need to go from a certain set of strings (the effects that applied) to a single one, through a sequence of handlers, monadic and comonadic rules and so on.
Notice that we never reference the zero-cells and that in particular their colors are purely an artistical touch\footnote{It looks nice and makes you forget this is applied abstract nonsense.}.
In the following sections, we will differentiate the different \emph{regions} of $\bar{\mC}$ by changing colours each time we cross a functor, which is actually an endofunctor in $\bar{\mC}$ and thus would not imply a colour change, as can be seen in Equations \eqref{eq:unitax} and \eqref{eq:muax}.
\begin{center}
	\inputtikz{sd-reduction-example}
\end{center}

\subsection{Achieving Normal Forms}
\label{subsec:normalforms}
We will now provide a set of rewriting rules on string diagrams (written as equations) which define the set of different possible reductions and explain how our typing rules compose diagrammatically.

\subsubsection{The Composition Dilemma}
\label{subsubsec:cds}
In Equation \eqref{eq:CSD} we provide an example of why composing string diagrams, in our paradigm, is not as simple as we would like.
However, we could solve this conundrum by adding to our set of equations the following rule, which we denote as the \emph{Curryfication} rule:
\begin{equation}
	\inputtikz{sd-curry}
	\label{eq:curry}\tag{Curryfication}
\end{equation}
This rule is to be seen as a straight-forward consequence of the isomorphism between $\mC\left( A, B\right)$
the arrows between $A$ and $B$ and $A \Rightarrow B$ the exponential object of $B$ to the $A$.
Allowing for that and remembering the fact that you can always replace a diagram by an equivalent one
before composing answers the question of how we create composition rules for string diagrams, or almost:
there is still a situation that is not accounted for, or at least explained:
what happens when applying an effectful diagram\footnote{When the associated effect is monadic this
	type of diagram represents an arrow in the Kleisli category of the monad.} to another effectful diagram?
Take the equation below:
\begin{equation*}
	\inputtikz{sd-kleisli-composition}
\end{equation*}

Note that the effects are applied in the order of the application, meaning that if we apply a function
$\phi: \tau_{1} \to \tau_{2} \to \tau_{3}$ to an element of type $F\tau_{1}$ then to an element of
type $F'\tau_{2}$ the result is of type $F'F\tau_{3}$ while applying in the other way around leads to
a result of type $FF'\tau_{3}$ (by considering the lambdas interexchangeable).

\subsubsection{Equations}
\label{subsubsec:sdq}
First, Theorem \ref{thm:isotopy} reminds the main result by \newcite{joyalGeometryTensorCalculus1991} about string diagrams which shows that our artistic representation of diagrams is correct and does not modify the equation or the rule we are presenting.
\begin{thm}[Theorem 3.1 \cite{selingerSurveyGraphicalLanguages2010}, Theorem 1.2 \cite{joyalGeometryTensorCalculus1991}]
	\label{thm:isotopy}
	A well-formed equation between morphism terms in the language of monoidal categories follows from the axioms of monoidal categories if and only if it holds, up to planar isotopy, in the graphical language.
\end{thm}
We will not prove this theorem here as it implies huge portions of graph theory and algebra which are out of the scope of this paper.

\medskip

Let us now look at a few of the equations that arise from the commutation of certain diagrams:
\begin{description}
	\item[The \emph{Elevator} Equations] are a consequence of Theorem \ref{thm:isotopy} but also highlight one of the most important properties of string diagrams in their modelisation of multi-threaded computations: what happens on one string does not influence what happens on another in the same time:
	      \begin{equation}
		      \inputtikz{sd-elevator}
		      \label{eq:elevator}
		      \tag{\href{https://î.fr/ascenseurs}{\emoji{elevator}}}
	      \end{equation}

	\item[The \emph{Snake} Equations] are a rewriting of the categorical diagrams equations \eqref{eq:zigzag} and \eqref{eq:zagzig} of Paragraph \ref{par:adjunctions} which are the defining properties of an adjunction.
	      If we have an adjunction $L\dashv R$:
	      \begin{equation}
		      \inputtikz{sd-snake1}
		      \tag{\rotatebox[origin=c]{90}{\emoji{snake}}}
		      \label{eq:snek1}
	      \end{equation}
	      \vspace{-12pt}
	      \begin{equation}
		      \inputtikz{sd-snake2}
		      \tag{\rotatebox[origin=c]{90}{\reflectbox{\emoji{snake}}}}
		      \label{eq:snek2}
	      \end{equation}
	      Note that we could express these equations using the following section, though it is, for now, a bit easier to keep it that way.
	\item[The \emph{(co-)Monadic} Equations] explain the very familiar sentence that a monad is just\footnote{Which is a monad} a monoid on the category of the endofunctors, that is, a multiplication law and a unit.
	      Here we do not present the co-monadic equations which are the exact same one but with a few letter changes and the up and down reversed.
	      \begin{equation}
		      \inputtikz{sd-monad-unit}
		      \label{eq:unitax}
		      \tag{$\eta$}
	      \end{equation}
	      \begin{equation}
		      \inputtikz{sd-monad-mult}
		      \label{eq:muax}
		      \tag{$\mu$}
	      \end{equation}
\end{description}
This set of equations, when added to our composition rules from Section \ref{subsubsec:cds} explain all the different reductions that can be made to limit non-determinism in our parsing strategies.
Indeed, considering the equivalence relation $\mathcal{R}$ freely generated from $\left\{ \eqref{eq:elevator}, \eqref{eq:snek1}, \eqref{eq:snek2}, \eqref{eq:muax}, \eqref{eq:unitax} \right\}$ and the equivalence relationship $\mathcal{R}'$ of planar isotopy from Theorem \ref{thm:isotopy}, we get a set of normal forms $\mathcal{N}$ from the set of all well-formed parsing diagrams $\mD$ defined by:
\begin{equation*}
	\mathcal{N} = \left( \mD / \mathcal{R} \right) / \mathcal{R}'
\end{equation*}

\subsection{Computing Normal Forms}
Now that we have a set of rules telling us what we can and cannot do in our model while preserving the equality of the diagrams, we provide a combinatorial description of our diagrams to help compute the possible equalities between multiple reductions of a sentence meaning.

\newcite{delpeuchNormalizationPlanarString2022} proposed a combinatorial description to check
in linear time for equality under Theorem \ref{thm:isotopy}.
However, this model does not suffice to account for all of our equations, especially as
labelling will influence the equations for monads, comonads and adjunctions.
However, adding edges to our data structure will render the linear time algorithm invalid and
probably provide problems that are solvable in polynomial time, but with their naive
algorithmic solution having a too large to compute complexity.

\section*{Acknowledgements}
Thanks to Antoine Groudiev for his precious insights on the direction the snakes for \eqref{eq:snek1} and \eqref{eq:snek2} should face.
\clearpage
\appendix
\section{Other Considered Things}
\subsection{Typing with a Product Category (and a bit of polymorphism)}
Another way to start would be to consider product categories: one for the main type system and one for the effects.
Let $\mC_{0}$ be a closed cartesian category representing our main type system.
Here we again consider constants and full computations as functions $\bot \to \tau$ or $\tau \in \mathrm{Obj}\left( \mC_{0} \right)$.
Now, to type functions and functors, we need to consider a second category:
We consider $\mC_{1}$ the category representing the free monoid on $\mF\left( \mL \right)$.
Monads and Applicatives will generate relations in that monoid.
To ease notation we will denote \emph{functor types} in $\mC_{1}$ as lists written with head on the left.

Finally, let $\mC = \mC_{0} \times \mC_{1}$ be the product category. This will be our typing category.
This means that the real type of objects will be $\left( \bot \to \tau, [] \right)$, which we will still denote by $\tau$.
We will denote by $F_{n} \cdots F_{0} \tau$ the type of an object, as if it were a composition of functions\footnote{It is!}.

In that paradigm, functors simply append to the head of the \emph{functor type} (with the same possible restrictions as before, though I do not see what they would be needed for) while functions will take a polymorphic form:
$x: L\tau_{1} \mapsto \phi x: L\tau_{2}$ and $\phi$'s type can be written as $\star\tau_{1} \to \star\tau_{2}$.

\bibliographystyle{main_natbib}
\bibliography{tdparse.bib}

\end{document}
