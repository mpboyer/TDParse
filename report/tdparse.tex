\documentclass[math, english, info]{cours}

\usepackage{bigstrut}
\usepackage{makecell}
\usepackage{tikz-dependency}

\def\cont{\Gamma\vdash}
\def\poulpe{\qquad}

\setlength\belowcaptionskip{0pt}
\setlength\abovecaptionskip{\baselineskip}

\DeclareMathOperator{\Var}{Var}
\makeatletter
\renewenvironment{thebibliography}[1]
     {\section{\bibname}
      \@mkboth{\MakeUppercase\bibname}{\MakeUppercase\bibname}%
      \list{\@biblabel{\@arabic\c@enumiv}}%
           {\settowidth\labelwidth{\@biblabel{#1}}%
            \leftmargin\labelwidth
            \advance\leftmargin\labelsep
            \@openbib@code
            \usecounter{enumiv}%
            \let\p@enumiv\@empty
            \renewcommand\theenumiv{\@arabic\c@enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
      \endlist}
\makeatother

\makeatletter
\def\black@or@white#1#2{%
  \@tempdima#2 pt
  \ifdim\@tempdima>0.5 pt
    \definecolor{temp@c}{gray}{0}%
  \else
    \definecolor{temp@c}{gray}{1}%
  \fi}
\def\letterbox#1#{\protect\letterb@x{#1}}
\def\letterb@x#1#2#3{%
  \colorlet{temp@c}[gray]{#2}%
  \extractcolorspec{temp@c}{\color@spec}%
  \expandafter\black@or@white\color@spec
  {\color#1{temp@c}\tallcbox#1{#2}{#3}}}
\def\tallcbox#1#{\protect\color@box{#1}}
\def\color@box#1#2{\color@b@x\relax{\color#1{#2}}}
\long\def\color@b@x#1#2#3%
 {\leavevmode
  \setbox\z@\hbox{{\set@color#3}}%
  \ht\z@\ht\strutbox
  \dp\z@\dp\strutbox
  {#1{#2\color@block{\wd\z@}{\ht\z@}{\dp\z@}\box\z@}}}
\makeatother

\contourlength{0.005em}
\def\backbox#1{\letterbox{Lavender!40}{\contour{black}{#1}}}

\def\ty#1{\backbox{\tt\color{yulm!90!black}#1}}
\def\f#1{\backbox{\tt\color{vulm}#1}}
\def\w#1{\mathbf{#1}\,}

\def\e{\ty{e}}
\def\t{\ty{t}}
\def\r{\ty{r}}

\newcolumntype{C}{>{$}c<{$}}
\newcolumntype{L}{>{$}l<{$}}
\newcolumntype{R}{>{$}r<{$}}
\def\fmap{\texttt{fmap}}


\makeatletter
\newcommand{\@word}[4][]{%
	#2 & #3 & #4\\
\ifx&#1&%
	%
\else
	&\multicolumn{2}{l}{Generalizes to \textbf{#1}}\\%
\fi%
}
\def\word#1#2#3#4{\@word[#4]{#1}{#2}{#3}}
\makeatother

\usepackage{calc}

\makeatletter
\def\textSq#1{%
\begingroup% make boxes and lengths local
\setlength{\fboxsep}{0.4ex}% SET ANY DESIRED PADDING HERE
\setbox1=\hbox{#1}% save the contents
\setlength{\@tempdima}{\maxof{\wd1}{\ht1+\dp1}}% size of the box
\setlength{\@tempdimb}{(\@tempdima-\ht1+\dp1)/2}% vertical raise
\raise-\@tempdimb\hbox{\fbox{\vbox to \@tempdima{%
  \vfil\hbox to \@tempdima{\hfil\copy1\hfil}\vfil}}}%
\endgroup%
}
\def\Sq#1{\textSq{\ensuremath{#1}}}%
\makeatother

\title{Formalizing Typing Rules for Natural Languages using Effects}
\author{Matthieu Boyer}

\begin{document}
\bettertitle

\begin{abstract}
The main idea is that you can consider some words to act as functors in a typing category, for example determiners:
they don't change the way a word acts in a sentence,
but more the setting in which the word works by adding an effect to the work.
This document is based mostly on \newcite{bumfordEffectdrivenInterpretation} and \newcite{bumfordEffectfulCompositionNatural}.
\end{abstract}

\section*{Introduction}
\newcite{moggiComputationalLambdacalculusMonads1989} provided a way to view monads as effects in functional programming.
This allows for new modes of combination in a compositional semantic formalism, and provides a way to model words which usually raise problems with the traditional lambda-calculus representation of the words.
In particular we consider words such as \textsl{the} or \textsl{a} whose application to common nouns results in types that should be used in similar situations but with largely different semantics, and model those as functions whose application yields an effect.
This allows us to develop typing judgements and an extended typing system for compositional semantics of natural languages.
Type-driven compositional semantics acts under the premise that given a set of words and their denotations, a set of grammar rules for composition and their associated typing judgements, we are able to form an enhanced parser for natural language which provides a mathematical representation of the meaning of sentences, as proposed by \newcite{heimSemanticsGenerativeGrammar1998}

\section{Categorical Semantics of Effects: A Typing System}
\label{sec:typingsystem}
In this section, we will designate by $\mL$ our language, as a set of words (with their semantics) and syntactic rules to combine them semantically.
We denote by $\O\left( \mL \right)$ the set of words in the language whose semantic representation is a low-order function and $\mF\left( \mL \right)$ the set of words whose semantic representation is a functor or high-order function.
Our goals here are to describe more formally, using a categorical vocabulary, the environment in which the typing system for our language will exist, and how we connect words and other linguistic objects to the categorical formulation.

\subsection{Typing Category}\label{subsec:typingcategory}
\subsubsection{Types}\label{subsubsec:types}
Let $\mC$ be a closed cartesian category. This represents our main typing system, consisting of words $\O(\mL)$ that can be expressed without effects.
Remember that $\mC$ contains a terminal object $\bot$ representing the empty type or the lack thereof.
We can consider $\bar{\mC}$ the category closure of $\mF\left( \mL \right)\left(\O\left( \mL \right)\right)$, that is consisting of all the different type constructors (ergo, functors) that could be formed in the language.
What this means is that we consider for our category objects any object that can be attained in a finite number from a finite number of functorial applications from an object of $\mC$.
In that sense, $\bar{\mC}$ is still a closed cartesian category (since all our functors induce isomorphisms on their image)\footnote{Our definition does not yield a closed cartesian category as there are no exponential objects between results of two different functors, but this is not a real issue as we can just say we add those.}.
$\bar{\mC}$ will be our typing category (in a way).

We consider for our types the quotient set $\star = \mathrm{Obj}\left( \bar{\mC} \right)/\mF\left( \mL \right)$.
Since $\mF\left( \mL \right)$ does not induce an equivalence relation on $\Obj\left( \bar{\mC} \right)$ but a preorder, we consider the chains obtained by the closure of the relation $x\succeq y \Leftrightarrow \exists F, y = F(x)$ (which is seen as a subtyping relation as proposed in \newcite{melliesFunctorsAreType2015}).
We also define $\star_{0}$ to be the set obtained when considering types which have not yet been \emph{affected}, that is $\Obj(\mC)$.
In contexts of polymorphism, we identify $\star_{0}$ to the adequate subset of $\star$.
In this paradigm, constant objects (or results of fully done computations) are functions with type $\bot \to \tau$ which we will denote directly by $\tau \in \star_{0}$.

What this construct actually means, in categorical terms, is that a type is an object of $\bar{\mC}$ (as intended) but we add a subtyping relationship based on the procedure used to construct $\bar{\mC}$.
Note that we can translate that subtyping relationship on functions as $F\left( A \xrightarrow{\phi} B \right)$ has types $F\left( A\Rightarrow B \right)$ and $FA \Rightarrow FB$.

We will provide in Table \ref{tab:sctypes} a list of the effect-less usual types associated to regular linguistic objects.

\subsubsection{Functors, Applicatives and Monads}\label{subsubsec:functors}
Our point of view leads us to consider \emph{language functors}\footnote{The elements of our language, not the categorical construct.} as polymorphic functions: for a (possibly restrained, though it seems to always be $\star$) set of base types $S$, a functor is a function
\begin{equation*}
	x: \tau\in S\subseteq \star \mapsto F x: F\tau
\end{equation*}
Remember that $\star$ is a fibration of the types in $\bar{\mC}$.
This means that if a functor can be applied to a type, it can also be applied to all \emph{affected} versions of that type, i.e. $\mF\left( L \right)(\tau\in \star)$.
More importantly, while it seems that $F$'s type is the identity on $\star$, the important part is that it changes the effects applied to $x$ (or $\tau$).
In that sense, $F$ has the following typing judgements:
\begin{equation*}
	\frac{\Gamma\vdash x: \tau \in \star_{0}}{\Gamma\vdash F x: F\tau \notin \star_{0}}\fracnotate{$\text{Func}_{0}$} \hspace{2cm} \frac{\Gamma\vdash x: \tau}{\Gamma\vdash Fx : F\tau\preceq \tau}\fracnotate{Func}
\end{equation*}
We use the same notation for the \emph{language functor} and the \emph{type functor} in the examples, but it is important to note those are two different objects, although connected.
More precisely, the \emph{language functor} is to be seen as a function whose computation yields an effect, while the \emph{type functor} is the endofunctor of $\bar{\mC}$ (so a functor from $\mC$) that represents the effect in our typing category.

In the same fashion, we can consider functions to have a type in $\star$ or more precisely of the form $\star \to \star$ which is a subset of $\star$.
This justifies how functors can act on functions in our typing system, thanks to the subtyping judgement introduced above, as this provides a way to ensure proper typing while just propagating the effects.
Because of propagation, this also means we can resolve the effects or keep on with the computation at any point during parsing, without any fear that the results may differ.

\medskip

In that sense, applicatives and monads only provide with more flexibility on the ways to combine functions:
they provide intermediate judgements to help with the combination of trees.
For example, the multiplication of the monad provides a new \emph{type conversion} judgement:
\begin{equation*}
	\frac{\Gamma\vdash x: MM\tau}{\Gamma\vdash x: M\tau \succeq MM\tau}\fracnotate{Monad}
\end{equation*}
This is actually a special case of the natural transformation rule that we define below, which means that, in a way, types $MM\star$ and $M\star$ are equivalent, as there is a canonical way to go from one type to another.
Remember however that $M\star$ is still a proper subtype of $MM\star$ and that the objects are not actually equal:
they are simply equivalent.

\subsubsection{Natural Transformations for Handlers and Higher-Order Constructs}\label{subsubsec:transnat}
We could also add judgements for adjunctions, but the most interesting thing is to add judgements for natural transformations, as adjunctions are particular examples of natural transformations which arise from \emph{natural} settings.
While in general we do not want to find natural transformations, we want to be able to express these in three situations:
\begin{enumerate}
	\item If we have an adjunction $L\dashv R$, we have natural transformations for $\Id_{\mC} \Rightarrow L \circ R$ and $R\circ L \Rightarrow \Id_{\mC}$.
		In particular we get a monad and a comonad from a canonical setting.
	\item To deal with the resolution of effects, we can map handlers to natural transformations which go from some functor $F$ to the $\Id$ functor, allowing for a sequential\footnote{In particular, non-necessarily commutative} computation of the effects added to the meaning.
		We will develop a bit more on this idea in \ref{par:handlers}.
	\item To create \emph{higher-order} constructs which transform words from our language into other words, while keeping the functorial aspect.
		This idea is developed in \ref{par:higherorder}.
\end{enumerate}
To see why we want this rule, which is a larger version of the monad multiplication and the monad/applicative unit, it suffices to see that the diagram below provides a way to construct the ``correct function'' on the ``correct functor'' side of types. If we have a natural transformation \begin{tikzcd}
	F\ar[r, Rightarrow, "\theta"] & G
\end{tikzcd} then for all arrows $f: \tau_{1} \to \tau_{2}$ we have:
\begin{category}
	F\tau_{1}\ar[r, "Ff"]\ar[d, "\theta_{\tau_{1}}"'] & F\tau_{2}\ar[d, "\theta_{\tau_{2}}"]\\
	G\tau_{1}\ar[r, "Gf"] & G\tau_{2}
\end{category}
and this implies, from it being true for all arrows, that from $\cont x: F\tau$ we have an easy construct to show $\cont x: G\tau$.

Remember that in the Haskell programming language, any polymorphic function is a natural transformation from the first type constructor to the second type constructor, as proved by \newcite{wadlerTheoremsFree1989}.
This will guarantee for us that given a \emph{Haskell} construction for a polymorphic function, we will get the associated natural transformation.

\paragraph{Adjunctions}
\label{par:adjunctions}
We will not go in much details about adjunctions, as a full example and generalization process is provided in \ref{subsec:effects}.
First, we remind the definition: an adjunction $L \dashv R$ is a pair of functors $L: \A \to \B$ and $R: \B \to \A$, and a pair of natural transformations $\eta: \Id_{\A}  \Rightarrow R \circ L$ and $\epsilon: L\circ R \Rightarrow \Id_{\A}$ such that the two following equations are satisfied:
	\begin{equation*}
		\begin{tikzcd}
			\A\ar[r, "\id_{\A}" name=idA]\ar[dr, "L"'] &[1.8cm] \A\ar[dr, "L"]\ar[phantom, "" name=A] &[1.8cm] \\[2cm]
		& \B\ar[u, "R"]\ar[phantom, "" name=B]\ddarrow["\Sq{\eta}"]{idA}{B}\ar[r, "\id_{B}" name=idB]\ddarrow["\text{\textcircled{$\epsilon$}}"]{A}{idB} & \B
	\end{tikzcd}
	= \begin{tikzcd}
		\A\ar[r, bend left=45, "L"' name=L]\ar[r, bend right=45, "L" name=R]\ddarrow["\id_{L}"]{L}{R}&[2cm] \B
	\end{tikzcd}
	\end{equation*}
	\begin{equation*}
	\begin{tikzcd}
		&[1.8cm] \A\ar[r, "\id_{\A}" name=idA]\ar[d, "L"'] \ar[phantom, "" name=A]&[1.8cm] \A\\[2cm]
		\B\ar[ur, "R"]\ar[r, "\id_{B}" name=idB]\ddarrow["\text{\textcircled{$\epsilon$}}"]{A}{idB} & \B\ar[ur, "R"]\ar[phantom, "" name=B] \ddarrow["\Sq{\eta}"]{idA}{B}
		&
	\end{tikzcd}
	=
	\begin{tikzcd}
		\B\ar[r, bend left=45, "R"' name=L]\ar[r, bend right=45, "R" name=R]\ddarrow["\id_{R}"]{L}{R}&[2cm] \A
	\end{tikzcd}
	\end{equation*}

An adjunction defines two different structures over itself: a monad $L \circ R$ and a comonad $R\circ L$.
The fact these structures arise from the interaction between two effects renders them an intrinsic property of the language.
In this lies the usefulness of adjunction in a typing system which uses effects: adjunctions provide a way to combine effects and to handle effects, allowing to simplify the computations on the free monoid on the set of functors.

\paragraph{Handlers}
\label{par:handlers}
As introduce by \newcite{marsikAlgebraicEffectsHandlers}, the use of handlers as annotations to the syntactic tree of the sentence is an appropriate formalism.
This could also give us a way to construct handlers for our effects as per \newcite{bauerEffectSystemAlgebraic2014}, or \newcite{plotkinHandlingAlgebraicEffects2013}.
As considered by \newcite{wuEffectHandlersScope2014} and \newcite{vandenbergFrameworkHigherorderEffects2024}, handlers are to be seen as natural transformations describing the free monad on an algebraic effect.
Considering handlers as so, allows us to directly handle our computations inside our typing system, by ``transporting'' our functors one order higher up without loss of information or generality since all our functors undergo the same transformation.
Using the framework proposed in \cite{vandenbergFrameworkHigherorderEffects2024} we simply need to create handlers for our effects/functors and we will then have in our language the result needed.

\medskip

What does this mean in our typing category ?
It means that either our language or our parser for the language \footnote{Depending whether we think handling effects is an intrinsic construct of the semantics of the language or whether it is associated with a speaker.} should contain natural transformations $F \Rightarrow \Id$ for $F \in \mF\left( \mL \right)$.
In this goal, we remember that from any polymorphic function in \emph{Haskell} we get a natural transformation \cite{wadlerTheoremsFree1989} meaning that it is enough to be able to define our handler in \emph{Haskell} to be ensured of its good definition.
Note that the choice of the handler being part of the lexicon or the parser over the other is a philosophical question more than a semantical one, as both options will result in semantically equivalent models, the only difference will be in the way we consider the resolution of effects.
Mathematically\footnote{Computer-wise, actually.}, this means the choice of either one of the options is purely of detail left during the implementation.

However, this choice does not arise in the case of the adjunction-induced handlers. Indeed here, the choice is caused by the non-uniqueness of the choices for the handlers.
For example, two different speakers may have different ways to resolve the $\f{S}$ (which will be introduced as the \emph{Powerset} monad in Section \ref{subsec:effects}) that arises from the phrase \textsl{A chair}.
This usual example of the differences between the cognitive representation of words is actually a specific example of the different possible handlers for the powerset representation of non-determinism/indefinites:
there are $\abs{S}$ arrows from the initial object to $S$ in $\mathit{Set}$, representing the different elements of $S$.
In that sense, while handlers may have a normal form or representation purely dependant on the effect, the actual handler does not necessarily have a canonical form.
This is the difference with the adjunctions: adjunctions are intrinsic properties of the coexistence of the effects\footnote{Which we identify to their functorial representations, which we may identify to their free monad in the framework \cite{vandenbergFrameworkHigherorderEffects2024}.}, while the handlers are user-defined.
As such, we choose to say that our handlers are implemented parser-side but again, this does not change our modelisation of handlers as natural transformations and most importantly, this does not add non-determinism to our model:
The non-determinism that arises from the variety of possible handlers does not add to the non-determinism in the parsing.

\paragraph{Higher-Order Constructs}
\label{par:higherorder}
We might want to add plurals, superlatives, tenses, aspects and other similar constructs which act as function modifiers.
For each of these, we give a functor $\Pi$ corresponding to a new class of types along with natural transformations for all other functors $F$ which allows to propagate down the high-order effect.
This transformation will need to be from $\Pi \circ F$ to $\Pi \circ F \circ \Pi$ or simply $\Pi \circ F \Rightarrow F \circ \Pi$ depending on the situation.
This allows us to add complexity not in the compositional aspects but in the lexicon aspects.
We do not want these functors to be applicatives, as we do not want a way to \emph{create} them in the parser if they are not marked in the sentence.

One of the main issues with this is the following:
In the English language, plural is marked on all words (except verbs, and even then case could be made for it to be marked),
while future is marked only on verbs (through the \textit{will + verb} construct which creates a ``\emph{new}'' verb in a sense) though it applies also to the arguments on the verb.
A way to solve this would be to include in the natural transformations rules to propagate the functor depending on the type of the object.
Consider the superlative effect \textbf{most}\footnote{We do not care about morphological markings here, we say that $\mathbf{largest} = \mathbf{most} \left(\mathbf{large}\right)$}.
As it can only be applied on adjectives, we can assume its argument is a function (but the definition would hold anyway taking $\tau_{1} = \bot$).
It is associated with the following function (which is a rewriting of the natural transformation rule):
\begin{equation*}
	\frac{\cont x: \tau_{1} \to \tau_{2}}{\cont \mathbf{most}\, x \coloneqq \Pi_{\tau_{2}} \circ x = x \circ \Pi_{\tau_{1}}}
\end{equation*}
What curryfication implies, is that higher-order constructs can be passed down to the arguments of the functions they are applied to, explaining how we can reconciliate the following semantic equation even if some of the words are not marked properly:
\begin{equation*}
	\bf future\left(be\left( I, a\ cat \right)\right) = will\ be\left( future\left( I \right), a\ cat \right) = be\left( future\left( I \right), a\ cat \right)
\end{equation*}
Indeed the above equation could be simply written by our natural transformation rule as:
\begin{equation*}
	\bf future\left( be \right)\left( arg_{1}, arg_{2} \right) = future\left( be \right)\left( arg_{2} \right)\left( future\left( arg_{1} \right) \right) = future \left( be \right) \left( future \left( arg_{2} \right) \right) \left( future \left( arg_{1} \right) \right)
\end{equation*}
This is not a definitive rule as we could want to stop at a step in the derivation, depending on our understanding of the notion of future in the language.

\subsection{Typing Judgements}\label{subsec:judgements}
To complete this section, Table \ref{tab:judgements} gives a simple list of different typing composition judgements through which we also re-derzive the subtyping judgement to allow for its implementation.
\begin{table}
\begin{align*}
	\forall F \in \mF\left( \mL \right),\poulpe
	\frac{\cont x: \tau \poulpe \cont F: S\subseteq \star \poulpe \overbrace{\tau \in S}^{\exists \tau'\in S, \tau \preceq \tau'}}{\cont Fx: F\tau \preceq \tau}\fracnotate{Cons}\\[.25cm]
	\frac{\cont x: \tau \poulpe \tau \in \star_{0}}{\cont Fx: F\tau \notin \star_{0}}\fracnotate{$FT_{0}$}\\[.25cm]
	\frac{\cont x: F\tau_{1} \poulpe \cont \phi: \tau_{1} \to \tau_{2}}{\cont \phi x: F\tau_{2} }\fracnotate{\texttt{fmap}}\\[.25cm]
	\frac{\cont x: \tau_{1} \poulpe \cont \phi: \tau_{1} \to \tau_{2}}{\cont \phi x: \tau_{2}}\fracnotate{App}\\[.5cm]
	\frac{\cont x: A\tau_{1} \poulpe \cont \phi: A\left( \tau_{1} \to \tau_{2} \right)}{\cont \phi x: A\tau_{2}}\fracnotate{\texttt{<*>}}\\[.25cm]
	\frac{\cont x: \tau}{\cont x: A\tau}\fracnotate{\texttt{pure/return}}\\[.5cm]
	\frac{\cont x: MM\tau}{\cont x: M\tau}\fracnotate{\texttt{>>=}}\\[.25cm]
	\forall F \overset{\theta}{\Longrightarrow} G,\poulpe \frac{\cont x: F\tau \poulpe \cont G: S' \subseteq \star \poulpe \tau \in S'}{\cont x : G\tau}\fracnotate{\texttt{nat}}
\end{align*}
\caption{Typing and Subtyping Judgements}
\label{tab:judgements}
\end{table}
Note that here, the syntax is not taken into account: a function is always written left of its arguments, whether or not they are actually in that order in the sentence.
This issue will be resolved by giving the syntactic tree of the sentence (or infering it at runtime).
We could also add symmetry induced rules for application.

Furthermore, note that the App rule is the \texttt{fmap} rule for the identity functor and that the \texttt{pure/return} and \texttt{>>=} is the \texttt{nat} rule for the monad unit (or applicative unit) and multiplication.

\medskip

Using these typing rules for our combinators, it is important to see that our grammar will still be ambiguous and thus our reduction process will be non-deterministic.
As an example, we provide the typing reductions for the classic example: \textsl{The man sees the girl using a telescope} in Figure \ref{fig:ud}.

\makeatletter
\def\c@lsep{2.3}
\def\r@wsep{.8}

\tikzset{
	uptree/.style={
		draw=green!80!black,
		thick,
	},
	typenode/.style={
		align=center,
		text width=24mm,
		%font={\large},
	},
	wordnode/.style={
		inner sep=0pt,
		align=center,
		font={\large},
	},
	downtree/.style={
		draw=red!80!black,
		thick,
	},
}

\newcommand{\wnode}[3]{%
	\node (#2) at (#1*\c@lsep, 0) [wordnode] {#2};
	\node[anchor=north] (#2-) at ($(#1*\c@lsep, 0) + (0, -.142)$) [typenode] {\ensuremath{#3}};
}
\newcommand{\utnode}[3]{%
	\path let \p1 = (#2.north), \p2 = (#3.north) in coordinate (Q1) at (\x1, {max(\y1, \y2)});
	\path let \p1 = (#2.north), \p2 = (#3.north) in coordinate (Q2) at (\x2, {max(\y1, \y2)});
	\node (#2#3) at ($($(Q1)!0.5!(Q2)$) + (0, 1)$) [typenode] {\ensuremath{#1}};
	\draw[uptree] ($(#2.north) + (0, .142)$) -- (#2#3.south);
	\draw[uptree] ($(#3.north) + (0, .142)$) -- (#2#3.south);
}
\newcommand{\dtnode}[4][0.5]{%
	\path let \p1 = (#3.south), \p2 = (#4.south) in coordinate (Q1) at (\x1, {min(\y1, \y2)});
	\path let \p1 = (#3.south), \p2 = (#4.south) in coordinate (Q2) at (\x2, {min(\y1, \y2)});
	\node (#3#4) at ($($(Q1)!#1!(Q2)$) + (0, -1)$) [typenode] {\ensuremath{#2}};
	\draw[downtree] ($(#3.south) + (0, -.142)$) -- (#3#4.north);
	\draw[downtree] ($(#4.south) + (0, -.142)$) -- (#3#4.north);
}
\makeatother

\begin{figure*}
\centering
\begin{tikzpicture}
	\wnode{0}{the}{\left( \e \to \t \right) \to \f{M}\e}
	\wnode{1}{man}{\left( \e \to \t \right)}
	\utnode{\f{M}\e}{the}{man}
	\dtnode{\f{M}\e}{the-}{man-}
	\wnode{2}{sees}{\left( \e \to \e \to \t \right)}
	\utnode{\f{M}\left( \e \to \t \right)}{theman}{sees}
	\dtnode{\f{M}\left( \e \to \t \right)}{the-man-}{sees-}
	\wnode{3}{the}{\left( \e \to \t \right) \to \f{M}\e}
	\wnode{4}{girl}{\left( \e \to \t \right)}
	\dtnode{\f{M}\e}{the-}{girl-}
	\wnode{5}{using}{\left( \e \to \t \right) \to \e \to \left( \e \to \t \right)}
	\wnode{6}{a}{\left( \e \to \t \right) \to \f{D}\e}
	\wnode{7}{telescope}{\left( \e \to \t \right)}
	\utnode{\f{D}\e}{a}{telescope}
	\dtnode{\f{D}\e}{a-}{telescope-}
	\utnode{\f{D}\left(\left( \e \to \t \right) \to \left( \e \to \t \right)\right)}{using}{atelescope}
	\dtnode{\f{D}\left(\left( \e \to \t \right) \to \left( \e \to \t \right)\right)}{using-}{a-telescope-}
	\utnode{\f{D}\left( \e \to \t \right)}{girl}{usingatelescope}
	\utnode{\f{M}\f{D}\e}{the}{girlusingatelescope}
	\utnode{\f{M}\f{M}\f{D}\t}{themansees}{thegirlusingatelescope}
	\utnode{\f{M}\f{D}\t}{themanseesthegirlusingatelescope}{themanseesthegirlusingatelescope}
	\dtnode[0.3]{\f{D}\f{M}\left( \e \to \t \right)}{the-man-sees-}{using-a-telescope-}
	\dtnode[1.2]{\f{D}\f{M}\f{M}\t}{the-man-sees-using-a-telescope-}{the-girl-}
	\dtnode{\f{D}\f{M}\t}{the-man-sees-using-a-telescope-the-girl-}{the-man-sees-using-a-telescope-the-girl-}
	%
	%
\end{tikzpicture}
  \caption{Parsing trees for the typing of \textsl{The man sees the girl using a telescope}.}
  \label{fig:ud}
\end{figure*}

This non-determinism is a component of our language's grammar and semantics: a same sentence can have multiple interpretation without context.
By the same reasoning we applied on handlers, we choose not to resolve the ambiguity in the parser but we will try in Section \ref{sec:nondet} to reduce it to a minimum.
Even so some of our derivations will include ways to read from a context (for example the \textbf{Jupiter, a planet} construct), we forget about the definition of the context, or its updating after a sentence, in our first proposition.

\section{Language Translation}
\label{sec:language}
In this section we will give a list of words along with a way to express them as either arrows or endo-functors of our typing category.
This will also give a set of functors and constructs in our language.

\subsection{Types of Syntax}\label{subsec:syntax}
We first need to setup some guidelines for our denotations, by asking ourselves how the different components of sentences interact with each other.
For syntactic categories, the types that are generally used are the following, and we will see it matches with our lexicon, and simplifies our functorial definitions\footnote{We don't consider effects in the given typings.}.
Those are based on \newcite{parteeLecture2Lambda}. Here $\Upsilon$ is the operator which retrieves the type from a certain syntactic category.
\begin{table}
	\centering
	\begin{minipage}{.7\textwidth}
\begin{multicols}{2}
\begin{description}
	\item[S] $\t$
	\item[CN(P)] $\e\to \t$
	\item[ProperN] $\e$
	\item[NP] Either:
		\begin{description}
			\item[$\e$] \textsl{John, a cat}
			\item[$\left(\e\to\t\right) \to \t$] \textsl{the cat, every man}
		\end{description}
	\item[DET] $\Upsilon\left( \mathbf{CN} \right) \to \Upsilon\left( \mathbf{NP} \right)$
	\item[ADJ(P)] Either:
		\begin{description}
			\item[$\left( \e \to \t \right)$] \textsl{carnivorous, happy}
			\item[$\left( \e \to \t \right) \to \left( \e \to \t \right)$] \textsl{skillful}
		\end{description}
	\item[REL] $\e \to \t$
	\item[VP, IV(P)] $\e \to \t$
	\item[TV(P)] $\Upsilon\left( \mathbf{NP} \right) \to \Upsilon\left( \mathbf{VP }\right)$
	\item[\it is] $\left( \e \to \t \right) \to \e \to \t$
\end{description}
\end{multicols}
\end{minipage}
\caption{Usual Typings for some Syntactic Categories}
\label{tab:sctypes}
\end{table}
\subsection{Lexicon: Semantic Denotations for Words}\label{subsec:lexicon}
Many words will have basically the same ``high-level'' denotation.
For example, the denotation for most common nouns will be of the form: $\cont \lambda x. \mathbf{planet} x: \e \to \t$.
In Table \ref{tab:lexicon} we give a lexicon for a subset of the english language.
We describe the constructor for the functors used by our denotations in the table, but all functors will be reminded and properly defined in Table \ref{tab:functors} along with their respective \fmap.
\begin{table}
	\centering
	\setcellgapes{2pt}
	\makegapedcells
	\begin{NiceTabular}{>{\bf}LLL}
		Expression & \rm Type & \lambda\text{-Term}\\
		\word{planet}{\e\to\t}{\lambda x. \w{planet} x}{common nouns}
		\word{carnivorous}{\left( \e \to \t \right)}{\lambda x. \w{carnivorous}x}{predicative adjectives}
		\word{skillful}{\left( \e \to \t \right) \to \left( \e \to \t \right)}{\lambda p. \lambda x. px \land \w{skillful} x}{predicate modifier adjectives}
		\word{Jupiter}{\e}{{\bf j}\in \Var}{proper nouns}
		\word{sleep}{\e \to \t}{\lambda x. \w{sleep} x}{intranitive verbs}
		\word{chase}{\e \to \e \to \t}{\lambda x. \lambda y. \mathbf{chase}\left( x, y \right)}{transitive verbs}
		\word{be}{\left( \e \to \t \right) \to \e \to \t}{\lambda p. \lambda x. px}{}
		\word{she}{\e\to\e}{\lambda x. x}{}
		\word{it}{\left( \bot \to \f{G}\e \right)}{\lambda g. g_{0}}{}
		\word{which}{\left( \e \to \t \right)\to \f{S}\e}{\lambda p. \left\{x \suchthat px\right\}}{}
		\word{the}{\left( \e \to \t \right) \to \f{M}\e}{\lambda p. x \text{ if } p = \{x\} \text{ else } \#}{}
		\word{a}{\left( \e \to \t \right) \to \f{D}\e}{\lambda p. \lambda s. \left\{ \scalar{x, x + s}\suchthat p x\right\}}{}
		\word{no}{\left( \e \to \t \right) \to \f{C}\e}{\lambda p. \lambda c. \lnot \exists x. p x \land c\, x}{}
		\word{every}{\left( \e \to \t \right)\to \f{C}\e}{\lambda p. \lambda c. \forall x, px \Rightarrow cx}{}
		\CodeAfter
		\begin{tikzpicture}
			\draw[double] (1|-2) -- (4|-2);
			\foreach \r in {4,6,...,14} {\draw (1|-\r) -- (4|-\r);}
			\foreach \r in {15,...,21} {\draw (1|-\r) -- (4|-\r);}
		\end{tikzpicture}
	\end{NiceTabular}
	\caption{$\lambda$-calculus representation of the english language $\mL$}
	\label{tab:lexicon}
\end{table}

\subsection{Effects of the Language}\label{subsec:effects}
For the applicatives/monads in Table \ref{tab:functors} we do not specify the unit and multiplication functions, as they are quite usual examples.
We still provide the \fmap{} for good measure.

\begin{table}
	\centering
	\def\arraystretch{1.3}
	\setcellgapes{2pt}
	\makegapedcells
	\begin{NiceTabular}{LLc}
		\rm Constructor & \fmap & Typeclass\\
		\f{G}\left( \tau \right) = \r \to \tau & \f{G}\phi\left( x \right) = \lambda r. \phi \left(x r\right) & Monad\\
		\f{W}\left( \tau \right) = \tau \times \t & \f{W}\phi\left( \left( a, p \right) \right) = \left( \phi a, p \right) & Monad\\
		\f{S}\left( \tau \right) = \{ \tau \} & \f{S}\phi\left( \left\{ x \right\} \right) = \left\{ \phi(x) \right\} & Monad\\
		\f{C}\left( \tau \right) = \left( \tau \to \t \right) \to \t & \f{C}\phi\left( x \right) = \lambda c. x\left( \lambda a. c \left( \phi a \right) \right) & Monad\\
		\f{M}\left( \tau \right) = \tau + \bot & \f{M}\phi\left( x \right) = \begin{cases}
			\phi\left( x \right) & \text{if } \cont x: \tau\\
			\# & \text{if } \cont x: \#
	\end{cases}& Monad\\
		\CodeAfter
		\begin{tikzpicture}
			\draw[double] (1|-2) -- (5|-2);
			\foreach\r in {3,...,6} {%
				\draw (1|-\r) -- (5|-\r);
			}
		\end{tikzpicture}
	\end{NiceTabular}
	\caption{Denotations for the functors used}
	\label{tab:functors}
\end{table}

Let us explain a few of those functors: $\f G$ designates reading from a certain environment of type $\r$ while $\f W$ encodes the possibility of logging a message of type $\t$ along with the expression.
The functor $\f M$ describes the possibility for a computation to fail, for example when retrieving a value that does not exist (see $\mathbf{the}$).
The $\f S$ functor represents the space of possibilities that arises from a non-deterministic computation (see $\mathbf{which}$).

We can then define an adjunction between $\f G$ and $\f W$ using our definitions:
\begin{equation*}
	\phi: \begin{array}{l}
		\left( \alpha \to \f{G}\beta \right)\to \f{W}\alpha \to \beta\\
		\lambda k. \lambda \left( a, g \right) . k a g
	\end{array}
	\text{ and }
	\psi: \begin{array}{l}
		\left( \f{W}\alpha \to \beta \right) \to \alpha \to \f{G} \beta\\
		\lambda c \lambda a \lambda g. c \left( a, g \right)
	\end{array}
\end{equation*}
where $\phi \circ \psi = \id$ and $\psi \circ \phi = \id$ on the properly defined sets.
Now it is easy to see that $\phi$ and $\psi$ do define a natural transformation and thus our adjunction $\f{W} \dashv \f{G}$ is well-defined, and that its unit $\eta$ is $\psi \circ \id$ and its co-unit $\epsilon$ is $\phi \circ \id$.

As a reminder, this means our language canonically defines a monad $\f{G}\circ \f{W}$.
Moreover, the co-unit of the adjunction provides a canonical way for us to deconstruct entirely the comonad $\f{W}\circ \f{G} $, that is we have a natural transformation from $\f{W} \circ \f{G} \Rightarrow \Id$.

\subsection{Modality, Plurality, Aspect, Tense}\label{subsec:modality}
We will now explain ways to formalise higher-order concepts as described in Section \ref{par:higherorder}.

First, let us consider in this example the plural concept.
Here we do not focus on whether our denotation of the plural is semantically correct, but simply on whether our modelisation makes sense.
As such, we give ourselves a way to measure if a certain $x$ is plural our not, which we will denote by $\abs{x} \geq 2$\footnote{We chose this representation as an example, it is not important for our formalism that this denotation is actually a good choice for the plural.}.
It is important to note that if $\tau$ is a type, and $\cont x: \tau$ then we should have $\abs{\Pi x} \geq 1 = \top\footnote{This might be seen as a typing judgement~!}$.
We then need to define the functor $\Pi$ which models the plural. We do not need to define it on types in any way other than $\Pi\left( \tau \right) = \Pi\tau$.
We only need to look at the transformation on arrows\footnote{\texttt{fmap}, basically.}.
This one depends on the \emph{syntactic}\footnote{Actually it depends on the word considered, but since the denotations (when considered effect-less) provided in Table \ref{tab:lexicon} are more or less related to the syntactic category of the word, our approximation suffices.} type of the arrow, as seen in Table \ref{tab:lexicon}.
\begin{table}
	\centering
	\def\arraystretch{1.3}
	\setcellgapes{3pt}
	\makegapedcells
	\begin{NiceTabular}{>{\bf}c>{\cont}C>{\Pi(p) = }L}
		CN(P) & p: \left(\e \to \t\right) & \lambda x.\left( px \land \abs{x} \geq 2 \right)\\
		\multirow{2}{*}{\bf ADJ(P)} & p: \left( \e \to \t \right) & \lambda x. \left( px \land \abs{x} \geq 2 \right)\\
		& p: \left( \e \to \t \right) \to \left( \e \to \t \right) & \lambda \nu. \lambda x. \left( p\left( \nu \right)\left( x \right) \land \abs{x} \geq 2 \right)\\
		\multirow{2}{*}{\bf NP} & p: \e & p \\
		& p: \left( \e \to \t \right) \to \t & \lambda \nu. p\left( \Pi \nu \right)\\
		IV(P)/VP & p: \e \to \t & \lambda o. \left( po \land \abs{x} \geq 2 \right)\\
		TV(P) & p: \e \to \e \to \t & \lambda s. \lambda o. \left( p\left( s \right)\left( o \right) \land \abs{s} \geq 2 \right)\\
		REL(P) & p: \e \to \t & \lambda x. \left( px \land \abs{x} \geq 2 \right)\\
		\multirow{2}{*}{\bf DET} & p: \left( \e \to \t \right) \to \left( \left( \e \to \t  \right) \to \t \right) & \lambda \nu. p\left( \Pi \nu \right)\\
		& p: \left( \e \to \t \right) \to \e & \lambda \nu. p\left( \Pi \nu \right)
		\CodeAfter
		\begin{tikzpicture}
			\foreach \r in {2,...,10} {\draw[dashed] ($(2|-\r) + (.1, 0)$) -- (4|-\r);}
			\foreach \r in {2, 4, 6, 7, 8, 9} {\draw (1|-\r) -- (4|-\r);}
			\draw (2|-1) -- (2|-11);
		\end{tikzpicture}
	\end{NiceTabular}
	\caption{(Partial) Definition for the $\Pi$ Plural Functor}
	\label{tab:pluralfunctor}
\end{table}
There is actually a presupposition in our definition. Whenever we apply $\Pi$ to an arrow representing a predicate $p: \e \to \t$, we still apply $p$ to its argument $x$ even though we say that $\Pi p$ is the one that applies to a plural entity $x$.
This slight notation abuse results from our point of view on plural/its predicate representation: we assume the predicate $p$ (the common noun, the adjectve, the VP\ldots) applies to an object without regarding its cardinality.
The two point of views on the singular/plural distinction could be adapted to our formalism: whether we believe that singular is the \emph{natural} state of the objects or that it is to be always specified in the same way as plural does not change anything:
In the former, we do not change anything from the proposed functors.
In the latter we simply need to create another functor $\Sigma$ with basically the same rules that represents the singular and ask of our predicates $p$ to be defined on $\left(\Pi\star\right) \sqcup \left(\Sigma \star\right)$.

\medskip

See that our functor only acts on the words which return a boolean by adding a plurality condition, and is simply passed down on the other words.
Note however there is an issue with the \textbf{NP} category: in the case where a \textbf{NP} is constructed from a determiner and a common noun (phrase) the plural is not passed down.
This comes from the fact that in this case, either the determiner or the common noun (phrase) will be marked, and by functoriality the plural will go up the tree.

Now clearly our functor verifies the identity and composition laws and works as theorised in Section \ref{par:higherorder}.
To understand a bit more why this works as described in our natural transformation rules, consider $\Pi\left( \mathbf{which} \right)$.
The result will be of type $\Pi\left( \f{S}\left( e \right) \right) = \left(\Pi \circ \f{S}\right) \left( e \right)$.
However, when looking at our transformation rule (and our functorial rule) we see that the result will actually be of type $\f{S}\left( \Pi e \right)$ which is exactly the expected type when considering the phrase $\w{which} p$.
The natural transformation $\theta$ we set is easily inferable:
\begin{equation*}
	\theta_{A}:
		\begin{tikzcd}
			\left( \Pi \circ \f{S} \right) A \ar[r, "\theta_{A}"] & \left( \f{S} \circ \Pi \right) A\\[-.7cm]
			\Pi\left( \left\{ x \right\} \right) \ar[r, mapsto] &  \left\{ \Pi(x) \right\}
		\end{tikzcd}
\end{equation*}
The naturality follows from the definition of $\fmap$ for the $\f{S}$ functor.
We can easily define natural transformations for the other functors in a similar way: $\Pi \circ \f{F} \overset{\theta}{\Rightarrow} \f{F} \circ \Pi$ defined by $\theta_{\tau} = \fmap_{\f{F}}\left( \Pi \right)$.

\medskip

Now, in quite a similar way, we can create functors from any sort of judgement that can be seen as a function $\e \to \t$ in our language\footnote{An easy way to define those would be, similarly to the plural, to define a series of judgement from a property that could be infered.}.
Indeed, we simply need to replace the $\abs{p} \geq 1$ in most of the definitions by our function and the rest would stay the same.
Remember that our use of natural transformations is only there to allow for possible under-markings of the considered effects and propagating the value resulting from the computation of the high-order effect.

\section{Solving Non-Determinism}\label{sec:nondet}
The typing judgements proposed in Section \ref{subsec:judgements} lead to ambiguity.
In this section we propose ways to get our derivations to a certain normal form, by deriving an equivalence relation on our derivation and parsing trees, based on string diagrams.

\subsection{String Diagrams}
String diagrams are the Poincar√© duals of the usual categorical diagrams when considered in the $2$-category of categories and functors.
In this category, the endofunctors are natural transformations.
This means that we represent categories as regions of the plane, functors as lines separating regions and natural transformations as the intersection points between two lines.
We will always consider application as applying to the right of the line.
This gives us a new graphical formalism to represent our effects using a few equality rules between diagrams.
The commutative aspect of functional diagrams is now replaced by an equality of string diagrams.

The important aspect of string diagrams that we will use is that two diagrams that are planarily homotopic are equal and that we can map a string diagram to a sequence of computations on computation: the vertical composition of natural transformations (bottom-up) represents the reductions that we can do on our set of effects.
This means that even when adding handlers, we get a way to visually see the meaning get reduced from effectful composition to propositional values, without the need to specify what the handler does.
Indeed we only look at \emph{when} we apply handlers and natural transformations reducing the number of effects.
This delimits our usage of string diagrams as ways to look at computations and a tool to provide equality rules to reduce non-determinism by constructing an equivalence relationship and yielding a quotient set on the computations to get \emph{normal forms}




\clearpage
\appendix
\section{Other Considered Things}
\subsection{Typing with a Product Category (and a bit of polymorphism)}
Another way to start would be to consider product categories: one for the main type system and one for the effects.
Let $\mC_{0}$ be a closed cartesian category representing our main type system.
Here we again consider constants and full computations as functions $\bot \to \tau$ or $\tau \in \mathrm{Obj}\left( \mC_{0} \right)$.
Now, to type functions and functors, we need to consider a second category:
We consider $\mC_{1}$ the category representing the free monoid on $\mF\left( \mL \right)$.
Monads and Applicatives will generate relations in that monoid.
To ease notation we will denote \emph{functor types} in $\mC_{1}$ as lists written with head on the left.

Finally, let $\mC = \mC_{0} \times \mC_{1}$ be the product category. This will be our typing category.
This means that the real type of objects will be $\left( \bot \to \tau, [] \right)$, which we will still denote by $\tau$.
We will denote by $F_{n} \cdots F_{0} \tau$ the type of an object, as if it were a composition of functions\footnote{It is!}.

In that paradigm, functors simply append to the head of the \emph{functor type} (with the same possible restrictions as before, though I do not see what they would be needed for) while functions will take a polymorphic form:
$x: L\tau_{1} \mapsto \phi x: L\tau_{2}$ and $\phi$'s type can be written as $\star\tau_{1} \to \star\tau_{2}$.

\bibliographystyle{main_natbib}
\bibliography{tdparse.bib}

\end{document}
