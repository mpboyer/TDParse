\section{Efficient Semantic Parsing}
\label{sec:parsing}
In this section we explain our algorithms and heuristics for efficient semantic
parsing with as little non-determinism as possible, and reducing time complexity
of our parsing strategies.

\subsection{Syntactic-Semantic Parsing}
Using a naïve strategy to compute judgements from the trees yields an
exponential algorithm, the best way to improve the algorithm, is to directly
leave the naïve strategy.
To do so, we could simply extend the grammar system used to do the syntactic
part of the parsing.
In this section, we will take the example of a CFG since it suffices to create
our typing combinators,
In Figure \ref{fig:combination-cfg}, we explicit a grammar of combination
modes, based on \cite{bumfordEffectdrivenInterpretationFunctors2025} as it
simplifies the rewriting of our typing judgements in a CFG.

\begin{figure}
	\centering
	\input{combination-cfg}
	\caption{Possible type combinations in the form of a near CFG}
	\label{fig:combination-cfg}
\end{figure}

This grammar works in five major sections:
\begin{enumerate}
	\item We reintroduce the grammar defining the type and effect system.
	\item We introduce a structure for the semantic parse trees and their labels,
	      the combination modes from
	      \cite{bumfordEffectdrivenInterpretationFunctors2025}.
	\item We introduce rules for basic type combinations.
	\item We introduce rules for higher-order unary type combinators.
	\item We introduce rules for higher-order binary type combinators.
\end{enumerate}

\begin{figure}[H]
	\centering
	\input{combinator-denotations}
	\caption{Denotations describing the effect of the combinators used in the
		grammar describing our combination modes presented in
		Figure \ref{fig:combination-cfg}}
	\label{fig:combinator-denotations}
\end{figure}

\begin{wrapfigure}[41]{l}{.45\textwidth}
	\centering
	\begin{subfigure}{.45\textwidth}
		\centering
		\resizebox{\textwidth}{!}{\begin{tikzpicture}[every tree node/.style={align=center, anchor=north}, level distance=2.5cm]
				\Tree [
				.{$\f{M}\f{D}\t$ \\ $\left\{\mathbf{eats}(\texttt{obj=}m, \texttt{subj=}c) \middle| \w{mouse}(m)\right\}$ if $\mathbf{cat}^{-1}(\top) = \{c\}$ \\ $\combMR_{\f{M}}\combML_{\f{D}}>$}
				[
				.{$\f{M}(\e)$ \\ $c$ if $\mathbf{cat}^{-1}(\top) = \{c\}$} \edge[roof]; {the cat}
				]
				[
				.{$\f{D}(\e \to \t)$ \\ $\left\{\lambda s. \mathbf{eats}(\texttt{obj=}m, \texttt{subj=}s) \middle| \w{mouse}(m)\right\}$} \edge[roof]; {eats a mouse}
				]
				]
			\end{tikzpicture}}
		\caption{Labelled tree representing the equivalent parsing diagram to
			\ref{fig:parsing-diagram}}
		\label{fig:tree-eats}
	\end{subfigure}

	\begin{subfigure}{.45\textwidth}
		\centering
		\begin{tikzpicture}[every tree node/.style={align=center, anchor=north}, level distance=2cm]
			\Tree [
			.{$\f{D}\e$ \\ $\{x \mid \w{cat} x \land \w{in\ a\ box} x \} $ \\ $\combJ_{\f{D}}\combML_{\f{D}} >$}
			{$(\e \to \t) \to \f{D}\e$ \\ a}
			[ .{$\f{D}(\e \to \t)$ \\ $\lambda x. \w{cat} x \land \w{in\ a\ box} x$} \edge[roof]; {cat in a box} ] ]
		\end{tikzpicture}
		\caption{Labelled tree representing the equivalent parsing diagram to
			\ref{fig:parsing-diagram2}}
		\label{fig:tree-box}
	\end{subfigure}

	\begin{subfigure}{.45\textwidth}
		\centering
		\resizebox{\textwidth}{!}{\begin{tikzpicture}[every tree node/.style={align=center, anchor=north}, level distance=1.75cm]
				\Tree [
				.\node{\t \\ $\mathbf{if}(\forall x. \w{past}\w{pass} x)(\w{past}\mathbf{rain})$ \\ $>$};
				[ .\node{$\t \to \t$ \\ $\mathbf{if}(\forall x.\w{past}\w{pass} x)$ \\ $>$};
				{$\t \to \t \to \t$ \\ if}
				[
				.\node{$\t$\\ $\forall x. \w{pass} x$ \\ $\combDN_{\Downarrow_{\f{C}}}$};
				[ .\node{$\f{C}\t$ \\ $\lambda c.\forall x. c(\w{past}\w{pass} x)$ \\ $\combMR_{\f{C}}<$};
				{$\f{C}\e$ \\ $\lambda c. \forall x. c\, x$ \\ everyone}
				{$\e \to \t$ \\ $\w{past}\mathbf{pass}$ \\ passed}
				]
				]
				]
				[
				.{\t \\ $\w{past}\mathbf{rain}$} \edge[roof]; {it was raining}
				]
				]
			\end{tikzpicture}}
		\caption{Labelled tree representing the equivalent parsing diagram to
			\ref{fig:3dparsing-diagram}}
		\label{fig:tree-rain}
	\end{subfigure}
	\caption{Examples of labeled parse trees for a few sentences.}
	\label{fig:parsing-trees}
\end{wrapfigure}

We do not prove here that these typing rules cannot be written in a simpler
syntactic structure, but it is easy to see why a regular expression would not
work, and since we need trees, to express our full system, the best we can do
would be to disambiguate the grammar.
Each of these combinators can be, up to order, associated with a inference
rule, and, as such, with a higher-order denotation, which explains the actual
effect of the combinator, and are described in
Figure \ref{fig:combinator-denotations}.


The main reason we need to get denotations associated to combinators, is to
properly define how they actually do the combination of denotations.
The important thing on those derivation is that they're a direct translation
of the rules defining the notions of functors, applicatives, monads and thus
are not specific to any denotation system, even though we will use
lambda-calculus styled denotations to describe them.

This makes us able to compute the actual denotations associated to a sentence
using our formalism, as presented in figure \ref{fig:parsing-trees}.
Note that the order of combination modes is not actually the same as the one
that would come from the grammar.
The reason why will become more apparent when string diagrams for parsing are
introduced in the next section, but simply, this comes from the fact that while
we think of $\combML$ and $\combMR$ as reducing the number of effects on each
side (and this is the correct way to think about those), this is not actually
how its denotation works, but there is no real issue with that.
This formalism gives us the following theorems:

\begin{theorem}
	\label{thm:ptime-parse}
	Semantic parsing of a sentence is polynomial in the length of the	sentence
	and the size of the type system and syntax system.
\end{theorem}

\begin{proof}
	Suppose we are given a syntactic generating structure $G_{s}$ along with our
	type combination grammar $G_{\tau}$.
	The system $G$ that can be constructed from the (tensor) product of $G_{s}$ and
	$G_{\tau}$ has size $\abs{G_{s}}\times \abs{G_{\tau}}$.
	Indeed, we think of its rules as a rule for the syntactic categories and a rule
	for the type combinations. Its terminals and non terminals are also in the
	cartesian products of the adequate sets in the original grammars.
	What this in turn means, is that if we can syntactically parse a sentence in
	polynomial time in the size of the input and in $\abs{G_{s}}$, then we can
	syntactico-semantically parse it in polynomial time in the size of the input,
	$\abs{G_{s}}$ and $\abs{G_{\tau}}$.
\end{proof}
\begin{wrapfigure}[31]{r}{.45\textwidth}
	\centering
	\includegraphics[width=.45\textwidth]{parsing-diagram}
	\caption{Representation of a parsing diagram for the sentence
		\emph{the cat eats a mouse}.
		See Figure \ref{fig:tree-box} for translation in a parse tree.}
	\label{fig:parsing-diagram}
\end{wrapfigure}

While we have gone with the hypothesis that we have a CFG for our language,
any type of polynomial-time structure could work, as long as it is more
expressive than a CFG.

The \emph{polynomial time combinators} assumption in the following theorem is
not a complex assumption, this is for example true for denotations based on
lambda-calculus, with function application being linear in the number of uses
of variables in the function term, which in turn is linear in the number of
terms used to construct the function term and thus of words, and the different
\fmap being in constant time for the same reason.

\begin{theorem}
	\label{thm:ptime-denot}
	Retrieving a pure denotation for a sentence can be done in polynomial time in
	the length of the sentence, given a polynomial time syntactic parsing
	algorithm and polynomial time combinators.
\end{theorem}

\begin{proof}
	We have proved in Theorem \ref{thm:ptime-parse} that we can retrieve a
	semantic parse tree from a	sentence in polynomial time in the input.
	Since we also have shown that the length of a semantic parse tree is quadratic
	in the length of the sentence it represents, being linear in the length of a
	syntactic parse tree linear in the length of the sentence.
	We have already seen that given a denotation, handling all effects and
	reducing effect handling to normal forms can be done in polynomial time.
	The superposition of these steps yields a polynomial-time algorithm in the
	length of the input sentence.
\end{proof}


\subsection{Diagrammatical Parsing}
\label{subsubsec:diagram-parsing}
When considering \cite{coeckeMathematicalFoundationsCompositional2010}
way of using string diagrams for syntactic parsing/reductions, we can see them
as (yet) another way of writing our parsing rules.
In our typed category, we can make see our combinators as natural
transformations ($2$-cells): then we can see the different sets of combinators
as different arity natural transformations.
$>$, $\combML_{\f{F}}$ and $\combJ_{\f{F}}$ are represented in
Figure \ref{fig:combinator-sd}, up to the coloring of the regions, because that
is purely for an artistic rendition.

\begin{figure}[t]
	\centering
	\input{combinators-sd}
	\caption{String Diagrammatic Representation of Combinator Modes $>, \combML$ and $\combJ$}
	\label{fig:combinator-sd}
\end{figure}

Now, a way to see this would be to think of this as an orthogonal set of
diagrams to the ones of Section \ref{sec:nondet}: we could use the syntactic
version of the diagrams to model our parsing, according to the rules in
Figure \ref{fig:combination-cfg}, and then combine the diagrams as shown in
Figure \ref{fig:parsing-diagram}, which highlights why we can consider the
diagrams orthogonal.
In this figure we exactly see the sequence of reductions play out on the types
of the words, and thus we also see what exact \emph{quasi-braiding} would be
needed to construct the effect diagram.
Here we talk about \emph{quasi-braiding} because, in a sense, we use $2$-cells
to do braiding-like operations on the strings, and don't actually allow for
braiding inside the diagrammatic computation.
To better understand what happens in those parsing diagrams, Figure
\ref{fig:parsing-trees} provides the translations in labelled trees of the
parsing diagrams of Figures \ref{fig:parsing-diagram},
\ref{fig:parsing-diagram2} and \ref{fig:3dparsing-diagram}.

\begin{wrapfigure}{r}{.45\textwidth}
	\centering
	\includegraphics[width=.45\textwidth]{parsing-diagram2.pdf}
	\caption{Example of a parsing diagram for the phrase
		\emph{a cat in a box}, presenting the integration of unary combinators
		inside the connector line. See Figure \ref{fig:tree-box} for translation in
		a parse tree.}
	\label{fig:parsing-diagram2}
\end{wrapfigure}

Categorically, what this means is that we start from a meaning category $\mC$,
our typing category, and take it as our grammatical category.
This is a form of extension on the monoidal version by
\cite{coeckeMathematicalFoundationsCompositional2010}, as it is seemingly a
typed version, where we change the pregroup category for the typing category,
possibly taken with a product for representation of the English grammar
representation, to accommodate for syntactic typing on top of semantic typing.
This is, again, just another rewriting of our typing rules.


We have a first axis of string diagrams in the category
$\mC$ - our string diagrams for effect handling, as in Section
\ref{sec:nondet} - and a second \emph{orthogonal} axis of string diagrams
on a larger category, with endofunctors labelled
by the types in our typing category $\bar{\mC}$ and
with natural transformations mapping the combinators defined in Figures
\ref{fig:combination-cfg} and \ref{fig:combinator-denotations}.
The category in which we consider the second-axis string diagrams does not have
a meaning in our compositional semantics theory, and to be more precise, we
should talk about $1$-cells and $2$-cells instead of functors and natural
transformations, to keep in the idea that this is really just a diagrammatic
way of computing and presenting the operations that are put to work during
semantic parsing:
A good approach to these diagrams is that they are an extension of the
labelled parse trees presented in
\cite{bumfordEffectdrivenInterpretationFunctors2025}, forming a full
diagrammatic calculus system for semantic parsing.

\smallskip

For the combinators $\combJ$, $\combDN$ and $\combC$, which are applied to
reduce the number of effects inside a denotation, it might seem less obvious
how to include them.
Applying them to the actual \emph{parsing} part of the diagram is done
in the exact same way as in the CFG: we just add them where needed, and they
will appear in the resulting denotation as a form of forced handling, in a
sense, as shown in the result of Figure \ref{fig:parsing-diagram2}.

\begin{wrapfigure}[14]{r}{.45\textwidth}
	\centering
	\begin{tikzpicture}
		\node (fig) at (0, 0) {\includegraphics[width=.4\textwidth]{knitting-example}};
		\draw[->] ($(fig.south west) + (-.1, -.1)$) -- node[anchor=east] {\rotatebox{90}{Direction of Reduction}} ($(fig.north west) + (-.1, .1)$);
	\end{tikzpicture}
	\caption{Example of a \emph{Jacquard} knitwork. Photography and work courtesy
		of the author's mother.}
	\label{fig:knitting-example}
\end{wrapfigure}

It is interesting to note that the resulting diagram representing
the sentence can visually be found in the connection strings that arise from
the combinators.
The main reason why this point of view of diagrammatic parsing is useful
will be clear when looking at the rewriting rules and the normal forms they
induce, because, as seen before, string diagrams make it easy to compute
normal forms, when provided with a confluent reduction system.

The other reason being the tangible interpretation of how things work
underlying the idea of a string diagram:
Suppose you're knitting a rainbow scarf.
You have multiple threads (the different words) of the different colours (their
types and effects) you're using to knit the scarf.

\begin{wrapfigure}[23]{l}{.5\textwidth}
	\centering
	\includegraphics[width=.5\textwidth]{3d-parsing-diagram}
	\caption{Knitting-like representation of the diagrammatic parsing of a sentence. See Figure \ref{fig:tree-rain} for the translation in a parse tree}
	\label{fig:3dparsing-diagram}
\end{wrapfigure}

When you decide to change the color you take the different threads you have
been using, and mix them up.
You can create a new colour\footnote{This is not how wool works, but
	if one can also imagine a pointillist-like way of drawing using multiple
	coloured lines that superimpose on each other, or a marching band's multiple
	instruments playing either in harmony or in disharmony and changing that
	during a score.} thread from two (that's the base combinators) create a
thicker one from two of the same colour (the applicative mode and the monadic
join), put aside a thread until a later step (that's the $\fmap$), add a new
thread to the pattern (that's the unit), or cut a thread you will not be
using anymore (that's the co-units and closure operators).
Changing a thread by cutting it and making a knot at another point is basically
what the eject combinators do.
This more tangible representation can be seen in a larger diagram in Figure
\ref{fig:3dparsing-diagram}.



The sections in the rectangle represent what happen when considering our
combination step as implementing patterns inside a knitwork, as seen in Figure
\ref{fig:knitting-example}.
The different patterns provide, in order, a visual representation of the
different ways one can combine two strings, i.e., two types and thus two
denotations.
The sections outside of the rectangle are the strings of yarn not currently
being used to make a pattern.

\subsection{Rewriting Rules}
\label{subsec:rewrite}
Here we provide a rewriting system on that allows us to
improve our time complexity by reducing the size of the grammar.
In the worst case, in big o notation there is no improvement in the size of the
sentence, but there is no loss.

\smallskip

First consider the case where we have presuppositions (or sub-trees) for the
two arguments of our parsing step, that are of type $\f{F}\tau$ and
$\f{G}\tau'$.
In that case we could either get a result with effects $\f{F}\f{G}$ or
with effects $\f{G}\f{F}$ (or possibly more).
In general those effects are not the same, but if they happen to be, which
trivially will be the case when one of the effects is external (the plural or
islands functors for example), the order of application does not matter and
thus we choose to get the outer effect the one of the left side of the
combinator.

\smallskip

Then there are sequence of modes that clearly encompass other ones
the grammar notation for ease of explanation.
One should not use the unit of a functor after using $\combML$ or $\combMR$, as
that adds void semantics.
Same things can be said for certain other derivations containing the lowering
and co-unit combinators:

\noindent We use $\combDN$ when we have not used any of the following, in all
derivations:
\begin{multicols}{2}
	\begin{itemize}
		\item $m_{\f{F}}, \combDN, m_{\f{F}}$ where
		      $m \in \{\combMR, \combML\}$
		\item $\combML_{\f{F}}, \combDN, \combMR_{\f{F}}$
		\item $\combA_{\f{F}}, \combDN, \combMR_{\f{F}}$
		\item $\combML_{\f{F}}, \combDN, \combA_{\f{F}}$
		\item $\combC$
	\end{itemize}
\end{multicols}
\noindent We use $\combJ$ if we have not used any of the following,
for $j \in \{\epsilon, \combJ_{\f{F}}\}$
\begin{multicols}{2}
	\begin{itemize}
		\item $\left\{m_{\f{F}}, j, m_{\f{F}}\right\}$ where
		      $m \in \{\combMR, \combML\}$
		\item $\combML_{\f{F}}, j, \combMR_{\f{f}}$
		\item $\combA_{\f{F}}, j, \combMR_{\f{F}}$,
		\item $\combML_{\f{F}}, j, \combA_{\f{F}}$
		\item $k, \combC$ for $k \in \{\epsilon, \combA_{\f{F}}\}$
		\item If $\f{F}$ is commutative as a monad:
		      \begin{itemize}
			      \item $\combMR_{\f{F}}, \combA_{\f{F}}$
			      \item $\combA_{\f{F}}, \combML_{\f{F}}$
			      \item $\combMR_{\f{F}}, j, \combML_{\f{F}}$
			      \item $\combA_{\f{F}}, j, \combA_{\f{F}}$
		      \end{itemize}
	\end{itemize}
\end{multicols}

\begin{theorem}
	The rules proposed above yield equivalent results.
\end{theorem}
\begin{proof}
	For the first point, it is obvious since based on an equality.
	For the second point:
	The rules about not using combinators $\combUL$ and $\combUR$ come from the
	notion of handling and granting termination and decidability to our system.
	The rules about adding $\combJ$ and $\combDN$ after moving two of the same
	effect from the same side (i.e. $\combML \combML$ or $\combMR\combMR$) are
	normalization of Theorem \ref{thm:isotopy}.
	Indeed, in the denotation, the only reason to keep two of the same effects
	and not join them is to at some point	have something get in between the two.
	Joining and closure should then be done at earliest point in parsing where t
	can be done, and that is equivalent to later points because of the elevator
	equations, or Theorem \ref{thm:isotopy}.
	The last set of rules follows from the following: we should not use $\combJ
		\combML \combMR$ instead of $\combA$, as those are equivalent because of the
	equation defining them.
	The same thing goes for the other two, as we should use the units of monads
	over applicative rules and \fmap.
\end{proof}

When using our diagrammatic approach to parsing, we can write all the
reductions described above to our paradigm: it simply amounts to
constructing a set of equational reductions for the string diagrams.
This leads to the same algorithms developed in Section \ref{sec:nondet} being
usable here: we just have a new improved version of Theorem
\ref{thm:confluence} which adds the normal forms specified in this section to
the newly added \emph{orthogonal} axis of diagrammatic computations.
What we're actually doing is computing two different normal forms along the
tensor product of our reduction schemes, which amounts to computing a larger
normal form.
Moreover, considering the possible normal forms of syntactic reductions
or denotational reductions adds ways to reduce our diagrams to normal forms.
Of course, this does not mean that our system is complete, as there might still
be many possible reductions to be found, although this would not reduce the
complexity of the algorithm and is more of a thought experiment.


